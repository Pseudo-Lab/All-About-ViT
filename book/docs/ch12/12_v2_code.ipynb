{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileViT V2 Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/rwightman/pytorch-image-models/blob/main/timm/models/mobilevit.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileVitV2Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileVitV2Block(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chs: int,\n",
    "        out_chs: Optional[int] = None,\n",
    "        kernel_size: int = 3,\n",
    "        bottle_ratio: float = 1.0,\n",
    "        group_size: Optional[int] = 1,\n",
    "        dilation: Tuple[int, int] = (1, 1),\n",
    "        mlp_ratio: float = 2.0,\n",
    "        transformer_dim: Optional[int] = None,\n",
    "        transformer_depth: int = 2,\n",
    "        patch_size: int = 8,\n",
    "        attn_drop: float = 0.,\n",
    "        drop: int = 0.,\n",
    "        drop_path_rate: float = 0.,\n",
    "        layers: LayerFn = None,\n",
    "        transformer_norm_layer: Callable = GroupNorm1,\n",
    "        **kwargs,  # eat unused args\n",
    "    ):\n",
    "        super(MobileVitV2Block, self).__init__()\n",
    "        layers = layers\n",
    "        groups = num_groups(group_size, in_chs)\n",
    "        out_chs = out_chs or in_chs\n",
    "        transformer_dim = transformer_dim\n",
    "\n",
    "        self.conv_kxk = layers.conv_norm_act(\n",
    "            in_chs, in_chs, kernel_size=kernel_size,\n",
    "            stride=1, groups=groups, dilation=dilation[0])\n",
    "        self.conv_1x1 = nn.Conv2d(in_chs, transformer_dim, kernel_size=1, bias=False)\n",
    "\n",
    "        self.transformer = nn.Sequential(*[\n",
    "            LinearTransformerBlock(\n",
    "                transformer_dim,\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                attn_drop=attn_drop,\n",
    "                drop=drop,\n",
    "                drop_path=drop_path_rate,\n",
    "                act_layer=layers.act,\n",
    "                norm_layer=transformer_norm_layer\n",
    "            )\n",
    "            for _ in range(transformer_depth)\n",
    "        ])\n",
    "        self.norm = transformer_norm_layer(transformer_dim)\n",
    "\n",
    "        self.conv_proj = layers.conv_norm_act(transformer_dim, out_chs, kernel_size=1, stride=1, apply_act=False)\n",
    "\n",
    "        self.patch_size = to_2tuple(patch_size)\n",
    "        self.patch_area = self.patch_size[0] * self.patch_size[1]\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, C, H, W = x.shape\n",
    "        patch_h, patch_w = self.patch_size\n",
    "        new_h, new_w = math.ceil(H / patch_h) * patch_h, math.ceil(W / patch_w) * patch_w\n",
    "        num_patch_h, num_patch_w = new_h // patch_h, new_w // patch_w  # n_h, n_w\n",
    "        num_patches = num_patch_h * num_patch_w  # N\n",
    "        if new_h != H or new_w != W:\n",
    "            x = F.interpolate(x, size=(new_h, new_w), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "        # Local representation\n",
    "        x = self.conv_kxk(x)\n",
    "        x = self.conv_1x1(x)\n",
    "\n",
    "        # Unfold (feature map -> patches), [B, C, H, W] -> [B, C, P, N]\n",
    "        C = x.shape[1]\n",
    "        x = x.reshape(B, C, num_patch_h, patch_h, num_patch_w, patch_w).permute(0, 1, 3, 5, 2, 4)\n",
    "        x = x.reshape(B, C, -1, num_patches)\n",
    "\n",
    "        # Global representations\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        # Fold (patches -> feature map), [B, C, P, N] --> [B, C, H, W]\n",
    "        x = x.reshape(B, C, patch_h, patch_w, num_patch_h, num_patch_w).permute(0, 1, 4, 2, 5, 3)\n",
    "        x = x.reshape(B, C, num_patch_h * patch_h, num_patch_w * patch_w)\n",
    "\n",
    "        x = self.conv_proj(x)\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearTransformerBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearTransformerBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        mlp_ratio: float = 2.0,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        act_layer=None,\n",
    "        norm_layer=None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        act_layer = act_layer or nn.SiLU\n",
    "        norm_layer = norm_layer or GroupNorm1\n",
    "\n",
    "        self.norm1 = norm_layer(embed_dim)\n",
    "        self.attn = LinearSelfAttention(embed_dim=embed_dim, attn_drop=attn_drop, proj_drop=drop)\n",
    "        self.drop_path1 = DropPath(drop_path)\n",
    "\n",
    "        self.norm2 = norm_layer(embed_dim)\n",
    "        self.mlp = ConvMlp(\n",
    "            in_features=embed_dim,\n",
    "            hidden_features=int(embed_dim * mlp_ratio),\n",
    "            act_layer=act_layer,\n",
    "            drop=drop)\n",
    "        self.drop_path2 = DropPath(drop_path)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if x_prev is None:\n",
    "            # self-attention\n",
    "            x = x + self.drop_path1(self.attn(self.norm1(x)))\n",
    "        else:\n",
    "            # cross-attention\n",
    "            res = x\n",
    "            x = self.norm1(x)  # norm\n",
    "            x = self.attn(x, x_prev)  # attn\n",
    "            x = self.drop_path1(x) + res  # residual\n",
    "\n",
    "        # Feed forward network\n",
    "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinearSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "        bias: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "        self.qkv_proj = nn.Conv2d(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=1 + (2 * embed_dim),\n",
    "            bias=bias,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.out_proj = nn.Conv2d(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=embed_dim,\n",
    "            bias=bias,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.out_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def _forward_self_attn(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # [B, C, P, N] --> [B, h + 2d, P, N]\n",
    "        qkv = self.qkv_proj(x)\n",
    "\n",
    "        # Project x into query, key and value\n",
    "        # Query --> [B, 1, P, N]\n",
    "        # value, key --> [B, d, P, N]\n",
    "        query, key, value = qkv.split([1, self.embed_dim, self.embed_dim], dim=1)\n",
    "\n",
    "        # apply softmax along N dimension\n",
    "        context_scores = F.softmax(query, dim=-1)\n",
    "        context_scores = self.attn_drop(context_scores)\n",
    "\n",
    "        # Compute context vector\n",
    "        # [B, d, P, N] x [B, 1, P, N] -> [B, d, P, N] --> [B, d, P, 1]\n",
    "        context_vector = (key * context_scores).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # combine context vector with values\n",
    "        # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]\n",
    "        out = F.relu(value) * context_vector.expand_as(value)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_drop(out)\n",
    "        return out\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def _forward_cross_attn(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        # x --> [B, C, P, N]\n",
    "        # x_prev = [B, C, P, M]\n",
    "        batch_size, in_dim, kv_patch_area, kv_num_patches = x.shape\n",
    "        q_patch_area, q_num_patches = x.shape[-2:]\n",
    "\n",
    "        assert (\n",
    "            kv_patch_area == q_patch_area\n",
    "        ), \"The number of pixels in a patch for query and key_value should be the same\"\n",
    "\n",
    "        # compute query, key, and value\n",
    "        # [B, C, P, M] --> [B, 1 + d, P, M]\n",
    "        qk = F.conv2d(\n",
    "            x_prev,\n",
    "            weight=self.qkv_proj.weight[:self.embed_dim + 1],\n",
    "            bias=self.qkv_proj.bias[:self.embed_dim + 1],\n",
    "        )\n",
    "\n",
    "        # [B, 1 + d, P, M] --> [B, 1, P, M], [B, d, P, M]\n",
    "        query, key = qk.split([1, self.embed_dim], dim=1)\n",
    "        # [B, C, P, N] --> [B, d, P, N]\n",
    "        value = F.conv2d(\n",
    "            x,\n",
    "            weight=self.qkv_proj.weight[self.embed_dim + 1],\n",
    "            bias=self.qkv_proj.bias[self.embed_dim + 1] if self.qkv_proj.bias is not None else None,\n",
    "        )\n",
    "\n",
    "        # apply softmax along M dimension\n",
    "        context_scores = F.softmax(query, dim=-1)\n",
    "        context_scores = self.attn_drop(context_scores)\n",
    "\n",
    "        # compute context vector\n",
    "        # [B, d, P, M] * [B, 1, P, M] -> [B, d, P, M] --> [B, d, P, 1]\n",
    "        context_vector = (key * context_scores).sum(dim=-1, keepdim=True)\n",
    "\n",
    "        # combine context vector with values\n",
    "        # [B, d, P, N] * [B, d, P, 1] --> [B, d, P, N]\n",
    "        out = F.relu(value) * context_vector.expand_as(value)\n",
    "        out = self.out_proj(out)\n",
    "        out = self.out_drop(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, x: torch.Tensor, x_prev: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        if x_prev is None:\n",
    "            return self._forward_self_attn(x)\n",
    "        else:\n",
    "            return self._forward_cross_attn(x, x_prev=x_prev)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Author by `김주영`  \n",
    "Edit by `김주영`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
