{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Transformer Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class patch_embedding(nn.Module) :\n",
    "    def __init__(self, patch_size, img_size, embed_size) :\n",
    "        super(patch_embedding, self).__init__()\n",
    "        \n",
    "        self.patch_embedding = nn.Conv2d(3, embed_size, \n",
    "                                         kernel_size=patch_size, \n",
    "                                         stride=patch_size)\n",
    "        # cls token을 패치 앞에 하나 더 붙여줌\n",
    "        self.cls_token = nn.Parameter(torch.rand(1,1,embed_size))\n",
    "        \n",
    "        # cls token 1개가 더 붙었기 때문에 총 patch 개수에 + 1을 해줌\n",
    "        self.position = nn.Parameter(torch.rand((img_size//patch_size)**2 + 1, embed_size))\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.patch_embedding(x)\n",
    "        x = x.flatten(2)\n",
    "        x = x.transpose(2,1)\n",
    "\n",
    "        ct = self.cls_token.repeat(x.shape[0], 1, 1)\n",
    "        x = torch.cat([ct, x],dim=1)\n",
    "        x += self.position\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_head_attention(nn.Module) :\n",
    "    def __init__(self, embed_size, num_head, dropout_rate=0.1) :\n",
    "        super(multi_head_attention, self).__init__()\n",
    "        \n",
    "        self.q = nn.Linear(embed_size, embed_size)\n",
    "        self.k = nn.Linear(embed_size, embed_size)\n",
    "        self.v = nn.Linear(embed_size, embed_size)\n",
    "        \n",
    "        self.fc = nn.Linear(embed_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.num_head = num_head\n",
    "        self.embed_size = embed_size\n",
    "    \n",
    "    def qkv_reshape(self, value, num_head) :\n",
    "        b, n, emb = value.size()\n",
    "        dim = emb // num_head\n",
    "        return value.view(b, num_head, n, dim)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        q = self.qkv_reshape(self.q(x), self.num_head)\n",
    "        k = self.qkv_reshape(self.k(x), self.num_head)\n",
    "        v = self.qkv_reshape(self.v(x), self.num_head)\n",
    "        \n",
    "        qk = torch.matmul(q, k.transpose(3,2))\n",
    "        att = F.softmax(qk / (self.embed_size ** (1/2)), dim=-1)\n",
    "        att = torch.matmul(att, v)\n",
    "        \n",
    "        b, h, n, d = att.size()\n",
    "        x = att.view(b, n, h*d)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module) :\n",
    "    def __init__(self, embed_size, expansion, dropout_rate):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(embed_size, embed_size*expansion)\n",
    "        self.fc2 = nn.Linear(embed_size*expansion, embed_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x) :\n",
    "        x = self.fc1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module) :\n",
    "    def __init__(self, \n",
    "                 embed_size, \n",
    "                 num_head, \n",
    "                 expansion, \n",
    "                 dropout_rate):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        \n",
    "        self.skip_connection1 = skip_connection(\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(embed_size),\n",
    "                multi_head_attention(embed_size, num_head, dropout_rate=0.1)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        self.skip_connection2 = skip_connection(\n",
    "            nn.Sequential(\n",
    "                nn.LayerNorm(embed_size),\n",
    "                MLP(embed_size, expansion, dropout_rate=0.1)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.skip_connection1(x)\n",
    "        x = self.skip_connection2(x)\n",
    "        return x\n",
    "\n",
    "class skip_connection(nn.Module) :\n",
    "\tdef __init__(self, layer):\n",
    "\t\tsuper(skip_connection, self).__init__()\n",
    "\t\tself.layer = layer\n",
    "\t\n",
    "\tdef forward (self, x):\n",
    "\t\treturn self.layer(x) + x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Head(nn.Module) :\n",
    "    def __init__(self, embed_size, num_classes):\n",
    "        super(Classifier_Head, self).__init__()\n",
    "        \n",
    "        self.avgpool1d = nn.AdaptiveAvgPool1d((1))\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, num_classes)\n",
    "        )\n",
    "\n",
    "\t  def forward(self, x) :\n",
    "        x = x.transpose(2,1)\n",
    "        x = self.avgpool1d(x).squeeze(2)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module) :\n",
    "    def __init__(self, \n",
    "                 patch_size=16, \n",
    "                 img_size=224, \n",
    "                 embed_size=768, \n",
    "                 num_head = 8,\n",
    "                 expansion = 4,\n",
    "                 dropout_rate = 0.1,\n",
    "                 encoder_depth = 12,\n",
    "                 num_classes = 10) :\n",
    "        super(VIT, self).__init__()\n",
    "\n",
    "        self.PatchEmbedding = patch_embedding(patch_size, img_size, embed_size)\n",
    "        self.EncoderBlocks = self.make_layers(encoder_depth, embed_size, num_head, expansion, dropout_rate)\n",
    "        self.ClassifierHead = Classifier_Head(embed_size, num_classes)\n",
    "        \n",
    "    def make_layers(self, encoder_depth, *args):\n",
    "        layers = []\n",
    "        for _ in range(0, encoder_depth) :\n",
    "            layers.append(EncoderBlock(*args))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        x = self.PatchEmbedding(x)\n",
    "        x = self.EncoderBlocks(x)\n",
    "        x = self.ClassifierHead(x)\n",
    "        \n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}