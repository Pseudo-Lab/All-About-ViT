{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MobileViT V3 Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileViTv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViTv3(BaseEncoder):\n",
    "\n",
    "    def __init__(self, opts, *args, **kwargs) -> None:\n",
    "        num_classes = getattr(opts, \"model.classification.n_classes\", 1000)\n",
    "        classifier_dropout = getattr(opts, \"model.classification.classifier_dropout\", 0.2)\n",
    "\n",
    "        pool_type = getattr(opts, \"model.layer.global_pool\", \"mean\")\n",
    "        image_channels = 3\n",
    "        out_channels = 16\n",
    "\n",
    "        mobilevit_config = get_configuration(opts=opts)\n",
    "\n",
    "        # Segmentation architectures like Deeplab and PSPNet modifies the strides of the classification backbones\n",
    "        # We allow that using `output_stride` arguments\n",
    "        output_stride = kwargs.get(\"output_stride\", None)\n",
    "        dilate_l4 = dilate_l5 = False\n",
    "        if output_stride == 8:\n",
    "            dilate_l4 = True\n",
    "            dilate_l5 = True\n",
    "        elif output_stride == 16:\n",
    "            dilate_l5 = True\n",
    "\n",
    "        super(MobileViTv3, self).__init__()\n",
    "        self.dilation = 1\n",
    "\n",
    "        # store model configuration in a dictionary\n",
    "        self.model_conf_dict = dict()\n",
    "        self.conv_1 = ConvLayer(\n",
    "                opts=opts, in_channels=image_channels, out_channels=out_channels,\n",
    "                kernel_size=3, stride=2, use_norm=True, use_act=True\n",
    "            )\n",
    "\n",
    "        self.model_conf_dict['conv1'] = {'in': image_channels, 'out': out_channels}\n",
    "\n",
    "        in_channels = out_channels\n",
    "        self.layer_1, out_channels = self._make_layer(\n",
    "            opts=opts, input_channel=in_channels, cfg=mobilevit_config[\"layer1\"]\n",
    "        )\n",
    "        self.model_conf_dict['layer1'] = {'in': in_channels, 'out': out_channels}\n",
    "\n",
    "        in_channels = out_channels\n",
    "        self.layer_2, out_channels = self._make_layer(\n",
    "            opts=opts, input_channel=in_channels, cfg=mobilevit_config[\"layer2\"]\n",
    "        )\n",
    "        self.model_conf_dict['layer2'] = {'in': in_channels, 'out': out_channels}\n",
    "\n",
    "        in_channels = out_channels\n",
    "        self.layer_3, out_channels = self._make_layer(\n",
    "            opts=opts, input_channel=in_channels, cfg=mobilevit_config[\"layer3\"]\n",
    "        )\n",
    "        self.model_conf_dict['layer3'] = {'in': in_channels, 'out': out_channels}\n",
    "\n",
    "        in_channels = out_channels\n",
    "        self.layer_4, out_channels = self._make_layer(\n",
    "            opts=opts, input_channel=in_channels, cfg=mobilevit_config[\"layer4\"], dilate=dilate_l4\n",
    "        )\n",
    "        self.model_conf_dict['layer4'] = {'in': in_channels, 'out': out_channels}\n",
    "\n",
    "        in_channels = out_channels\n",
    "        self.layer_5, out_channels = self._make_layer(\n",
    "            opts=opts, input_channel=in_channels, cfg=mobilevit_config[\"layer5\"], dilate=dilate_l5\n",
    "        )\n",
    "        self.model_conf_dict['layer5'] = {'in': in_channels, 'out': out_channels}\n",
    "\n",
    "        in_channels = out_channels\n",
    "        exp_channels = min(mobilevit_config[\"last_layer_exp_factor\"] * in_channels, 960)\n",
    "        self.conv_1x1_exp = ConvLayer(\n",
    "                opts=opts, in_channels=in_channels, out_channels=exp_channels,\n",
    "                kernel_size=1, stride=1, use_act=True, use_norm=True\n",
    "            )\n",
    "\n",
    "        self.model_conf_dict['exp_before_cls'] = {'in': in_channels, 'out': exp_channels}\n",
    "\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module(name=\"global_pool\", module=GlobalPool(pool_type=pool_type, keep_dim=False))\n",
    "        if 0.0 < classifier_dropout < 1.0:\n",
    "            self.classifier.add_module(name=\"dropout\", module=Dropout(p=classifier_dropout, inplace=True))\n",
    "        self.classifier.add_module(\n",
    "            name=\"fc\",\n",
    "            module=LinearLayer(in_features=exp_channels, out_features=num_classes, bias=True)\n",
    "        )\n",
    "\n",
    "        # check model\n",
    "        self.check_model()\n",
    "\n",
    "        # weight initialization\n",
    "        self.reset_parameters(opts=opts)\n",
    "\n",
    "    @classmethod\n",
    "    def add_arguments(cls, parser: argparse.ArgumentParser):\n",
    "        group = parser.add_argument_group(title=\"\".format(cls.__name__), description=\"\".format(cls.__name__))\n",
    "        group.add_argument('--model.classification.mit.mode', type=str, default=None,\n",
    "                           choices=['xx_small', 'x_small', 'small'], help=\"MIT mode\")\n",
    "        group.add_argument('--model.classification.mit.attn-dropout', type=float, default=0.1,\n",
    "                           help=\"Dropout in attention layer\")\n",
    "        group.add_argument('--model.classification.mit.ffn-dropout', type=float, default=0.0,\n",
    "                           help=\"Dropout between FFN layers\")\n",
    "        group.add_argument('--model.classification.mit.dropout', type=float, default=0.1,\n",
    "                           help=\"Dropout in Transformer layer\")\n",
    "        group.add_argument('--model.classification.mit.transformer-norm-layer', type=str, default=\"layer_norm\",\n",
    "                           help=\"Normalization layer in transformer\")\n",
    "        group.add_argument('--model.classification.mit.no-fuse-local-global-features', action=\"store_true\",\n",
    "                           help=\"Do not combine local and global features in MIT block\")\n",
    "        group.add_argument('--model.classification.mit.conv-kernel-size', type=int, default=3,\n",
    "                           help=\"Kernel size of Conv layers in MIT block\")\n",
    "\n",
    "        group.add_argument('--model.classification.mit.head-dim', type=int, default=None,\n",
    "                           help=\"Head dimension in transformer\")\n",
    "        group.add_argument('--model.classification.mit.number-heads', type=int, default=None,\n",
    "                           help=\"No. of heads in transformer\")\n",
    "        return parser\n",
    "\n",
    "    def _make_layer(self, opts, input_channel, cfg: Dict, dilate: Optional[bool] = False) -> Tuple[nn.Sequential, int]:\n",
    "        block_type = cfg.get(\"block_type\", \"mobilevit\")\n",
    "        if block_type.lower() == \"mobilevit\":\n",
    "            return self._make_mit_layer(\n",
    "                opts=opts,\n",
    "                input_channel=input_channel,\n",
    "                cfg=cfg,\n",
    "                dilate=dilate\n",
    "            )\n",
    "        else:\n",
    "            return self._make_mobilenet_layer(\n",
    "                opts=opts,\n",
    "                input_channel=input_channel,\n",
    "                cfg=cfg\n",
    "            )\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_mobilenet_layer(opts, input_channel: int, cfg: Dict) -> Tuple[nn.Sequential, int]:\n",
    "        output_channels = cfg.get(\"out_channels\")\n",
    "        num_blocks = cfg.get(\"num_blocks\", 2)\n",
    "        expand_ratio = cfg.get(\"expand_ratio\", 4)\n",
    "        block = []\n",
    "\n",
    "        for i in range(num_blocks):\n",
    "            stride = cfg.get(\"stride\", 1) if i == 0 else 1\n",
    "\n",
    "            layer = InvertedResidual(\n",
    "                opts=opts,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=output_channels,\n",
    "                stride=stride,\n",
    "                expand_ratio=expand_ratio\n",
    "            )\n",
    "            block.append(layer)\n",
    "            input_channel = output_channels\n",
    "        return nn.Sequential(*block), input_channel\n",
    "\n",
    "    def _make_mit_layer(self, opts, input_channel, cfg: Dict, dilate: Optional[bool] = False) -> Tuple[nn.Sequential, int]:\n",
    "        prev_dilation = self.dilation\n",
    "        block = []\n",
    "        stride = cfg.get(\"stride\", 1)\n",
    "\n",
    "        if stride == 2:\n",
    "            if dilate:\n",
    "                self.dilation *= 2\n",
    "                stride = 1\n",
    "\n",
    "            layer = InvertedResidual(\n",
    "                opts=opts,\n",
    "                in_channels=input_channel,\n",
    "                out_channels=cfg.get(\"out_channels\"),\n",
    "                stride=stride,\n",
    "                expand_ratio=cfg.get(\"mv_expand_ratio\", 4),\n",
    "                dilation=prev_dilation\n",
    "            )\n",
    "\n",
    "            block.append(layer)\n",
    "            input_channel = cfg.get(\"out_channels\")\n",
    "\n",
    "        head_dim = cfg.get(\"head_dim\", 32)\n",
    "        transformer_dim = cfg[\"transformer_channels\"]\n",
    "        ffn_dim = cfg.get(\"ffn_dim\")\n",
    "        if head_dim is None:\n",
    "            num_heads = cfg.get(\"num_heads\", 4)\n",
    "            if num_heads is None:\n",
    "                num_heads = 4\n",
    "            head_dim = transformer_dim // num_heads\n",
    "\n",
    "        if transformer_dim % head_dim != 0:\n",
    "            logger.error(\"Transformer input dimension should be divisible by head dimension. \"\n",
    "                         \"Got {} and {}.\".format(transformer_dim, head_dim))\n",
    "\n",
    "        block.append(\n",
    "            MobileViTv3Block(\n",
    "                opts=opts,\n",
    "                in_channels=input_channel,\n",
    "                transformer_dim=transformer_dim,\n",
    "                ffn_dim=ffn_dim,\n",
    "                n_transformer_blocks=cfg.get(\"transformer_blocks\", 1),\n",
    "                patch_h=cfg.get(\"patch_h\", 2),\n",
    "                patch_w=cfg.get(\"patch_w\", 2),\n",
    "                dropout=getattr(opts, \"model.classification.mit.dropout\", 0.1),\n",
    "                ffn_dropout=getattr(opts, \"model.classification.mit.ffn_dropout\", 0.0),\n",
    "                attn_dropout=getattr(opts, \"model.classification.mit.attn_dropout\", 0.1),\n",
    "                head_dim=head_dim,\n",
    "                no_fusion=getattr(opts, \"model.classification.mit.no_fuse_local_global_features\", False),\n",
    "                conv_ksize=getattr(opts, \"model.classification.mit.conv_kernel_size\", 3)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return nn.Sequential(*block), input_channel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileViTv3Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViTv3Block(BaseModule):\n",
    "    \"\"\"\n",
    "        MobileViTv3 block\n",
    "    \"\"\"\n",
    "    def __init__(self, opts, in_channels: int, transformer_dim: int, ffn_dim: int,\n",
    "                 n_transformer_blocks: Optional[int] = 2,\n",
    "                 head_dim: Optional[int] = 32, attn_dropout: Optional[float] = 0.1,\n",
    "                 dropout: Optional[int] = 0.1, ffn_dropout: Optional[int] = 0.1, patch_h: Optional[int] = 8,\n",
    "                 patch_w: Optional[int] = 8, transformer_norm_layer: Optional[str] = \"layer_norm\",\n",
    "                 conv_ksize: Optional[int] = 3,\n",
    "                 dilation: Optional[int] = 1, var_ffn: Optional[bool] = False,\n",
    "                 no_fusion: Optional[bool] = False,\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        # For MobileViTv3: Normal 3x3 convolution --> Depthwise 3x3 convolution\n",
    "        conv_3x3_in = ConvLayer(\n",
    "            opts=opts, in_channels=in_channels, out_channels=in_channels,\n",
    "            kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True, dilation=dilation,\n",
    "            groups=in_channels\n",
    "        )\n",
    "        conv_1x1_in = ConvLayer(\n",
    "            opts=opts, in_channels=in_channels, out_channels=transformer_dim,\n",
    "            kernel_size=1, stride=1, use_norm=False, use_act=False\n",
    "        )\n",
    "\n",
    "\n",
    "        conv_1x1_out = ConvLayer(\n",
    "            opts=opts, in_channels=transformer_dim, out_channels=in_channels,\n",
    "            kernel_size=1, stride=1, use_norm=True, use_act=True\n",
    "        )\n",
    "        conv_3x3_out = None\n",
    "\n",
    "        # For MobileViTv3: input+global --> local+global\n",
    "        if not no_fusion:\n",
    "            #input_ch = tr_dim + in_ch\n",
    "            conv_3x3_out = ConvLayer(\n",
    "                opts=opts, in_channels= transformer_dim + in_channels, out_channels=in_channels,\n",
    "                kernel_size=1, stride=1, use_norm=True, use_act=True\n",
    "            )\n",
    "\n",
    "        super(MobileViTv3Block, self).__init__()\n",
    "        self.local_rep = nn.Sequential()\n",
    "        self.local_rep.add_module(name=\"conv_3x3\", module=conv_3x3_in)\n",
    "        self.local_rep.add_module(name=\"conv_1x1\", module=conv_1x1_in)\n",
    "\n",
    "        assert transformer_dim % head_dim == 0\n",
    "        num_heads = transformer_dim // head_dim\n",
    "\n",
    "        ffn_dims = [ffn_dim] * n_transformer_blocks\n",
    "\n",
    "        global_rep = [\n",
    "            TransformerEncoder(opts=opts, embed_dim=transformer_dim, ffn_latent_dim=ffn_dims[block_idx], num_heads=num_heads,\n",
    "                               attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout,\n",
    "                               transformer_norm_layer=transformer_norm_layer)\n",
    "            for block_idx in range(n_transformer_blocks)\n",
    "        ]\n",
    "        global_rep.append(\n",
    "            get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=transformer_dim)\n",
    "        )\n",
    "        self.global_rep = nn.Sequential(*global_rep)\n",
    "\n",
    "        self.conv_proj = conv_1x1_out\n",
    "\n",
    "        self.fusion = conv_3x3_out\n",
    "\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "        self.patch_area = self.patch_w * self.patch_h\n",
    "\n",
    "        self.cnn_in_dim = in_channels\n",
    "        self.cnn_out_dim = transformer_dim\n",
    "        self.n_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.dropout = dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.dilation = dilation\n",
    "        self.ffn_max_dim = ffn_dims[0]\n",
    "        self.ffn_min_dim = ffn_dims[-1]\n",
    "        self.var_ffn = var_ffn\n",
    "        self.n_blocks = n_transformer_blocks\n",
    "        self.conv_ksize = conv_ksize\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = \"{}(\".format(self.__class__.__name__)\n",
    "        repr_str += \"\\n\\tconv_in_dim={}, conv_out_dim={}, dilation={}, conv_ksize={}\".format(self.cnn_in_dim, self.cnn_out_dim, self.dilation, self.conv_ksize)\n",
    "        repr_str += \"\\n\\tpatch_h={}, patch_w={}\".format(self.patch_h, self.patch_w)\n",
    "        repr_str += \"\\n\\ttransformer_in_dim={}, transformer_n_heads={}, transformer_ffn_dim={}, dropout={}, \" \\\n",
    "                    \"ffn_dropout={}, attn_dropout={}, blocks={}\".format(\n",
    "            self.cnn_out_dim,\n",
    "            self.n_heads,\n",
    "            self.ffn_dim,\n",
    "            self.dropout,\n",
    "            self.ffn_dropout,\n",
    "            self.attn_dropout,\n",
    "            self.n_blocks\n",
    "        )\n",
    "        if self.var_ffn:\n",
    "            repr_str += \"\\n\\t var_ffn_min_mult={}, var_ffn_max_mult={}\".format(\n",
    "                self.ffn_min_dim, self.ffn_max_dim\n",
    "            )\n",
    "\n",
    "        repr_str += \"\\n)\"\n",
    "        return repr_str\n",
    "\n",
    "    def unfolding(self, feature_map: Tensor) -> Tuple[Tensor, Dict]:\n",
    "        patch_w, patch_h = self.patch_w, self.patch_h\n",
    "        patch_area = int(patch_w * patch_h)\n",
    "        batch_size, in_channels, orig_h, orig_w = feature_map.shape\n",
    "\n",
    "        new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)\n",
    "        new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)\n",
    "\n",
    "        interpolate = False\n",
    "        if new_w != orig_w or new_h != orig_h:\n",
    "            # Note: Padding can be done, but then it needs to be handled in attention function.\n",
    "            feature_map = F.interpolate(feature_map, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n",
    "            interpolate = True\n",
    "\n",
    "        # number of patches along width and height\n",
    "        num_patch_w = new_w // patch_w # n_w\n",
    "        num_patch_h = new_h // patch_h # n_h\n",
    "        num_patches = num_patch_h * num_patch_w # N\n",
    "\n",
    "        # [B, C, H, W] --> [B * C * n_h, p_h, n_w, p_w]\n",
    "        reshaped_fm = feature_map.reshape(batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w)\n",
    "        # [B * C * n_h, p_h, n_w, p_w] --> [B * C * n_h, n_w, p_h, p_w]\n",
    "        transposed_fm = reshaped_fm.transpose(1, 2)\n",
    "        # [B * C * n_h, n_w, p_h, p_w] --> [B, C, N, P] where P = p_h * p_w and N = n_h * n_w\n",
    "        reshaped_fm = transposed_fm.reshape(batch_size, in_channels, num_patches, patch_area)\n",
    "        # [B, C, N, P] --> [B, P, N, C]\n",
    "        transposed_fm = reshaped_fm.transpose(1, 3)\n",
    "        # [B, P, N, C] --> [BP, N, C]\n",
    "        patches = transposed_fm.reshape(batch_size * patch_area, num_patches, -1)\n",
    "\n",
    "        info_dict = {\n",
    "            \"orig_size\": (orig_h, orig_w),\n",
    "            \"batch_size\": batch_size,\n",
    "            \"interpolate\": interpolate,\n",
    "            \"total_patches\": num_patches,\n",
    "            \"num_patches_w\": num_patch_w,\n",
    "            \"num_patches_h\": num_patch_h\n",
    "        }\n",
    "\n",
    "        return patches, info_dict\n",
    "\n",
    "    def folding(self, patches: Tensor, info_dict: Dict) -> Tensor:\n",
    "        n_dim = patches.dim()\n",
    "        assert n_dim == 3, \"Tensor should be of shape BPxNxC. Got: {}\".format(patches.shape)\n",
    "        # [BP, N, C] --> [B, P, N, C]\n",
    "        patches = patches.contiguous().view(info_dict[\"batch_size\"], self.patch_area, info_dict[\"total_patches\"], -1)\n",
    "\n",
    "        batch_size, pixels, num_patches, channels = patches.size()\n",
    "        num_patch_h = info_dict[\"num_patches_h\"]\n",
    "        num_patch_w = info_dict[\"num_patches_w\"]\n",
    "\n",
    "        # [B, P, N, C] --> [B, C, N, P]\n",
    "        patches = patches.transpose(1, 3)\n",
    "\n",
    "        # [B, C, N, P] --> [B*C*n_h, n_w, p_h, p_w]\n",
    "        feature_map = patches.reshape(batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w)\n",
    "        # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w]\n",
    "        feature_map = feature_map.transpose(1, 2)\n",
    "        # [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]\n",
    "        feature_map = feature_map.reshape(batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w)\n",
    "        if info_dict[\"interpolate\"]:\n",
    "            feature_map = F.interpolate(feature_map, size=info_dict[\"orig_size\"], mode=\"bilinear\", align_corners=False)\n",
    "        return feature_map\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "\n",
    "        # For MobileViTv3: Normal 3x3 convolution --> Depthwise 3x3 convolution\n",
    "        fm_conv = self.local_rep(x)\n",
    "\n",
    "        # convert feature map to patches\n",
    "        patches, info_dict = self.unfolding(fm_conv)\n",
    "\n",
    "        # learn global representations\n",
    "        patches = self.global_rep(patches)\n",
    "\n",
    "        # [B x Patch x Patches x C] --> [B x C x Patches x Patch]\n",
    "        fm = self.folding(patches=patches, info_dict=info_dict)\n",
    "\n",
    "        fm = self.conv_proj(fm)\n",
    "\n",
    "        if self.fusion is not None:\n",
    "            # For MobileViTv3: input+global --> local+global\n",
    "            fm = self.fusion(\n",
    "                torch.cat((fm_conv, fm), dim=1)\n",
    "            )\n",
    "\n",
    "        # For MobileViTv3: Skip connection\n",
    "        fm = fm + res\n",
    "\n",
    "        return fm\n",
    "\n",
    "    def profile_module(self, input: Tensor) -> (Tensor, float, float):\n",
    "        params = macs = 0.0\n",
    "\n",
    "        res = input\n",
    "        out_conv, p, m = module_profile(module=self.local_rep, x=input)\n",
    "        params += p\n",
    "        macs += m\n",
    "\n",
    "        patches, info_dict = self.unfolding(feature_map=out_conv)\n",
    "\n",
    "        patches, p, m = module_profile(module=self.global_rep, x=patches)\n",
    "        params += p\n",
    "        macs += m\n",
    "\n",
    "        fm = self.folding(patches=patches, info_dict=info_dict)\n",
    "\n",
    "        out, p, m = module_profile(module=self.conv_proj, x=fm)\n",
    "        params += p\n",
    "        macs += m\n",
    "\n",
    "        if self.fusion is not None:\n",
    "            out, p, m = module_profile(module=self.fusion, x=torch.cat((out, out_conv), dim=1))\n",
    "            params += p\n",
    "            macs += m\n",
    "\n",
    "        return res, params, macs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileViTv3Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileViTv3Block(BaseModule):\n",
    "    \"\"\"\n",
    "        MobileViTv3 block\n",
    "    \"\"\"\n",
    "    def __init__(self, opts, in_channels: int, transformer_dim: int, ffn_dim: int,\n",
    "                 n_transformer_blocks: Optional[int] = 2,\n",
    "                 head_dim: Optional[int] = 32, attn_dropout: Optional[float] = 0.1,\n",
    "                 dropout: Optional[int] = 0.1, ffn_dropout: Optional[int] = 0.1, patch_h: Optional[int] = 8,\n",
    "                 patch_w: Optional[int] = 8, transformer_norm_layer: Optional[str] = \"layer_norm\",\n",
    "                 conv_ksize: Optional[int] = 3,\n",
    "                 dilation: Optional[int] = 1, var_ffn: Optional[bool] = False,\n",
    "                 no_fusion: Optional[bool] = False,\n",
    "                 *args, **kwargs):\n",
    "\n",
    "        # For MobileViTv3: Normal 3x3 convolution --> Depthwise 3x3 convolution\n",
    "        conv_3x3_in = ConvLayer(\n",
    "            opts=opts, in_channels=in_channels, out_channels=in_channels,\n",
    "            kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True, dilation=dilation,\n",
    "            groups=in_channels\n",
    "        )\n",
    "        conv_1x1_in = ConvLayer(\n",
    "            opts=opts, in_channels=in_channels, out_channels=transformer_dim,\n",
    "            kernel_size=1, stride=1, use_norm=False, use_act=False\n",
    "        )\n",
    "\n",
    "\n",
    "        conv_1x1_out = ConvLayer(\n",
    "            opts=opts, in_channels=transformer_dim, out_channels=in_channels,\n",
    "            kernel_size=1, stride=1, use_norm=True, use_act=True\n",
    "        )\n",
    "        conv_3x3_out = None\n",
    "\n",
    "        # For MobileViTv3: input+global --> local+global\n",
    "        if not no_fusion:\n",
    "            #input_ch = tr_dim + in_ch\n",
    "            conv_3x3_out = ConvLayer(\n",
    "                opts=opts, in_channels= transformer_dim + in_channels, out_channels=in_channels,\n",
    "                kernel_size=1, stride=1, use_norm=True, use_act=True\n",
    "            )\n",
    "\n",
    "        super(MobileViTv3Block, self).__init__()\n",
    "        self.local_rep = nn.Sequential()\n",
    "        self.local_rep.add_module(name=\"conv_3x3\", module=conv_3x3_in)\n",
    "        self.local_rep.add_module(name=\"conv_1x1\", module=conv_1x1_in)\n",
    "\n",
    "        assert transformer_dim % head_dim == 0\n",
    "        num_heads = transformer_dim // head_dim\n",
    "\n",
    "        ffn_dims = [ffn_dim] * n_transformer_blocks\n",
    "\n",
    "        global_rep = [\n",
    "            TransformerEncoder(opts=opts, embed_dim=transformer_dim, ffn_latent_dim=ffn_dims[block_idx], num_heads=num_heads,\n",
    "                               attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout,\n",
    "                               transformer_norm_layer=transformer_norm_layer)\n",
    "            for block_idx in range(n_transformer_blocks)\n",
    "        ]\n",
    "        global_rep.append(\n",
    "            get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=transformer_dim)\n",
    "        )\n",
    "        self.global_rep = nn.Sequential(*global_rep)\n",
    "\n",
    "        self.conv_proj = conv_1x1_out\n",
    "\n",
    "        self.fusion = conv_3x3_out\n",
    "\n",
    "        self.patch_h = patch_h\n",
    "        self.patch_w = patch_w\n",
    "        self.patch_area = self.patch_w * self.patch_h\n",
    "\n",
    "        self.cnn_in_dim = in_channels\n",
    "        self.cnn_out_dim = transformer_dim\n",
    "        self.n_heads = num_heads\n",
    "        self.ffn_dim = ffn_dim\n",
    "        self.dropout = dropout\n",
    "        self.attn_dropout = attn_dropout\n",
    "        self.ffn_dropout = ffn_dropout\n",
    "        self.dilation = dilation\n",
    "        self.ffn_max_dim = ffn_dims[0]\n",
    "        self.ffn_min_dim = ffn_dims[-1]\n",
    "        self.var_ffn = var_ffn\n",
    "        self.n_blocks = n_transformer_blocks\n",
    "        self.conv_ksize = conv_ksize\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = \"{}(\".format(self.__class__.__name__)\n",
    "        repr_str += \"\\n\\tconv_in_dim={}, conv_out_dim={}, dilation={}, conv_ksize={}\".format(self.cnn_in_dim, self.cnn_out_dim, self.dilation, self.conv_ksize)\n",
    "        repr_str += \"\\n\\tpatch_h={}, patch_w={}\".format(self.patch_h, self.patch_w)\n",
    "        repr_str += \"\\n\\ttransformer_in_dim={}, transformer_n_heads={}, transformer_ffn_dim={}, dropout={}, \" \\\n",
    "                    \"ffn_dropout={}, attn_dropout={}, blocks={}\".format(\n",
    "            self.cnn_out_dim,\n",
    "            self.n_heads,\n",
    "            self.ffn_dim,\n",
    "            self.dropout,\n",
    "            self.ffn_dropout,\n",
    "            self.attn_dropout,\n",
    "            self.n_blocks\n",
    "        )\n",
    "        if self.var_ffn:\n",
    "            repr_str += \"\\n\\t var_ffn_min_mult={}, var_ffn_max_mult={}\".format(\n",
    "                self.ffn_min_dim, self.ffn_max_dim\n",
    "            )\n",
    "\n",
    "        repr_str += \"\\n)\"\n",
    "        return repr_str\n",
    "\n",
    "    def unfolding(self, feature_map: Tensor) -> Tuple[Tensor, Dict]:\n",
    "        patch_w, patch_h = self.patch_w, self.patch_h\n",
    "        patch_area = int(patch_w * patch_h)\n",
    "        batch_size, in_channels, orig_h, orig_w = feature_map.shape\n",
    "\n",
    "        new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)\n",
    "        new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)\n",
    "\n",
    "        interpolate = False\n",
    "        if new_w != orig_w or new_h != orig_h:\n",
    "            # Note: Padding can be done, but then it needs to be handled in attention function.\n",
    "            feature_map = F.interpolate(feature_map, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n",
    "            interpolate = True\n",
    "\n",
    "        # number of patches along width and height\n",
    "        num_patch_w = new_w // patch_w # n_w\n",
    "        num_patch_h = new_h // patch_h # n_h\n",
    "        num_patches = num_patch_h * num_patch_w # N\n",
    "\n",
    "        # [B, C, H, W] --> [B * C * n_h, p_h, n_w, p_w]\n",
    "        reshaped_fm = feature_map.reshape(batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w)\n",
    "        # [B * C * n_h, p_h, n_w, p_w] --> [B * C * n_h, n_w, p_h, p_w]\n",
    "        transposed_fm = reshaped_fm.transpose(1, 2)\n",
    "        # [B * C * n_h, n_w, p_h, p_w] --> [B, C, N, P] where P = p_h * p_w and N = n_h * n_w\n",
    "        reshaped_fm = transposed_fm.reshape(batch_size, in_channels, num_patches, patch_area)\n",
    "        # [B, C, N, P] --> [B, P, N, C]\n",
    "        transposed_fm = reshaped_fm.transpose(1, 3)\n",
    "        # [B, P, N, C] --> [BP, N, C]\n",
    "        patches = transposed_fm.reshape(batch_size * patch_area, num_patches, -1)\n",
    "\n",
    "        info_dict = {\n",
    "            \"orig_size\": (orig_h, orig_w),\n",
    "            \"batch_size\": batch_size,\n",
    "            \"interpolate\": interpolate,\n",
    "            \"total_patches\": num_patches,\n",
    "            \"num_patches_w\": num_patch_w,\n",
    "            \"num_patches_h\": num_patch_h\n",
    "        }\n",
    "\n",
    "        return patches, info_dict\n",
    "\n",
    "    def folding(self, patches: Tensor, info_dict: Dict) -> Tensor:\n",
    "        n_dim = patches.dim()\n",
    "        assert n_dim == 3, \"Tensor should be of shape BPxNxC. Got: {}\".format(patches.shape)\n",
    "        # [BP, N, C] --> [B, P, N, C]\n",
    "        patches = patches.contiguous().view(info_dict[\"batch_size\"], self.patch_area, info_dict[\"total_patches\"], -1)\n",
    "\n",
    "        batch_size, pixels, num_patches, channels = patches.size()\n",
    "        num_patch_h = info_dict[\"num_patches_h\"]\n",
    "        num_patch_w = info_dict[\"num_patches_w\"]\n",
    "\n",
    "        # [B, P, N, C] --> [B, C, N, P]\n",
    "        patches = patches.transpose(1, 3)\n",
    "\n",
    "        # [B, C, N, P] --> [B*C*n_h, n_w, p_h, p_w]\n",
    "        feature_map = patches.reshape(batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w)\n",
    "        # [B*C*n_h, n_w, p_h, p_w] --> [B*C*n_h, p_h, n_w, p_w]\n",
    "        feature_map = feature_map.transpose(1, 2)\n",
    "        # [B*C*n_h, p_h, n_w, p_w] --> [B, C, H, W]\n",
    "        feature_map = feature_map.reshape(batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w)\n",
    "        if info_dict[\"interpolate\"]:\n",
    "            feature_map = F.interpolate(feature_map, size=info_dict[\"orig_size\"], mode=\"bilinear\", align_corners=False)\n",
    "        return feature_map\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        res = x\n",
    "\n",
    "        # For MobileViTv3: Normal 3x3 convolution --> Depthwise 3x3 convolution\n",
    "        fm_conv = self.local_rep(x)\n",
    "\n",
    "        # convert feature map to patches\n",
    "        patches, info_dict = self.unfolding(fm_conv)\n",
    "\n",
    "        # learn global representations\n",
    "        patches = self.global_rep(patches)\n",
    "\n",
    "        # [B x Patch x Patches x C] --> [B x C x Patches x Patch]\n",
    "        fm = self.folding(patches=patches, info_dict=info_dict)\n",
    "\n",
    "        fm = self.conv_proj(fm)\n",
    "\n",
    "        if self.fusion is not None:\n",
    "            # For MobileViTv3: input+global --> local+global\n",
    "            fm = self.fusion(\n",
    "                torch.cat((fm_conv, fm), dim=1)\n",
    "            )\n",
    "\n",
    "        # For MobileViTv3: Skip connection\n",
    "        fm = fm + res\n",
    "\n",
    "        return fm\n",
    "\n",
    "    def profile_module(self, input: Tensor) -> (Tensor, float, float):\n",
    "        params = macs = 0.0\n",
    "\n",
    "        res = input\n",
    "        out_conv, p, m = module_profile(module=self.local_rep, x=input)\n",
    "        params += p\n",
    "        macs += m\n",
    "\n",
    "        patches, info_dict = self.unfolding(feature_map=out_conv)\n",
    "\n",
    "        patches, p, m = module_profile(module=self.global_rep, x=patches)\n",
    "        params += p\n",
    "        macs += m\n",
    "\n",
    "        fm = self.folding(patches=patches, info_dict=info_dict)\n",
    "\n",
    "        out, p, m = module_profile(module=self.conv_proj, x=fm)\n",
    "        params += p\n",
    "        macs += m\n",
    "\n",
    "        if self.fusion is not None:\n",
    "            out, p, m = module_profile(module=self.fusion, x=torch.cat((out, out_conv), dim=1))\n",
    "            params += p\n",
    "            macs += m\n",
    "\n",
    "        return res, params, macs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "Author by `정영상`  \n",
    "Edit by `김주영`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}