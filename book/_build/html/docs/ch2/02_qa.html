
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Q&amp;A &#8212; Vision Transformer의 모든 것</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Pyramid Vision Transformer" href="../ch3/03_List.html" />
    <link rel="prev" title="Vision Transformer Code" href="02_code.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PseudoLab_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Vision Transformer의 모든 것</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Vision Transformer의 모든 것
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Inductive Bias와 Self-Attention
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch1/01_List.html">
   Inductive Bias와 Self-Attention
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/Inductive_Bias.html">
     Inductive Bias
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/Self-Attention.html">
     Self-Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/01_code.html">
     Self-Attention Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/01_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Vision Transformer
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="02_List.html">
   Vision Transformer
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="vit.html">
     Vision Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_code.html">
     Vision Transformer Code
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pyramid Vision Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch3/03_List.html">
   Pyramid Vision Transformer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/Pyramid_Vision_Transformer.html">
     Pyramid Vision Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_code.html">
     Pyramid Vision Transformer Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DeiT
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch4/04_List.html">
   DeiT
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/DeiT.html">
     DeiT: Training data-efficient image transformers &amp; distillation through attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/04_code.html">
     DeiT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch4/04_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tokens-to-Token ViT
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch5/05_List.html">
   Tokens-to-Token ViT
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/T2T-ViT.html">
     Tokens-to-Token ViT
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/05_code.html">
     Tokens-to-Token ViT Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch5/05_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  BEiT
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch6/06_List.html">
   BEiT: BERT Pre-Training of Image Transformers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/BEiT.html">
     BEiT: BERT Pre-Training of Image Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/06_code.html">
     BEiT: BERT Pre-Training of Image Transformers Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch6/06_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  SepViT
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch7/07_List.html">
   SepViT: Separable Vison Transformer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/SepViT.html">
     SepViT: Separable Vison Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/07_code.html">
     SepViT: Separable Vison Transformer Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch7/07_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Compact Convolutional Transformers
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch8/08_List.html">
   Compact Convolutional Transformers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch8/CCT.html">
     Compact Convolutional Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch8/08_code.html">
     Compact Convolutional Transformers Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch8/08_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Compact Vision Transformers
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch9/09_List.html">
   Compact Vision Transformers
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/CvT.html">
     Compact Vision Transformers
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/09_code.html">
     Compact Vision Transformers Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch9/09_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Swin Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch10/10_List.html">
   Swin Transformer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/Swin_Transformer.html">
     Swin Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/10_code.html">
     Swin Transformer Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch10/10_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Vision Transformer with Deformable Attention
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch11/11_List.html">
   Vision Transformer with Deformable Attention
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch11/Vision_Transformer_with_Deformable_Attention.html">
     Vision Transformer with Deformable Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch11/11_code.html">
     Vision Transformer with Deformable Attention Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch11/11_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/pseudo-lab/All-About-ViT"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/pseudo-lab/All-About-ViT/issues/new?title=Issue%20on%20page%20%2Fdocs/ch2/02_qa.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/docs/ch2/02_qa.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gelu">
   GELU를 사용한 이유
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cls-token-pe">
   cls_token와 pe를 추가하는 순서
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fine-tuning-and-higher-resolution">
   논문 3.2절 FINE-TUNING AND HIGHER RESOLUTION
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pe-vit-learnable">
   왜 PE가 ViT에서 learnable한 파라미터로 구성되어있는가
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#patch-size">
   논문 3.2절 patch size를 증가/감소시키는 것에 따른 장단점
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-distance">
   4.5절의 attention distance의 의미
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Q&A</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#gelu">
   GELU를 사용한 이유
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cls-token-pe">
   cls_token와 pe를 추가하는 순서
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fine-tuning-and-higher-resolution">
   논문 3.2절 FINE-TUNING AND HIGHER RESOLUTION
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pe-vit-learnable">
   왜 PE가 ViT에서 learnable한 파라미터로 구성되어있는가
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#patch-size">
   논문 3.2절 patch size를 증가/감소시키는 것에 따른 장단점
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#attention-distance">
   4.5절의 attention distance의 의미
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="q-a">
<h1>Q&amp;A<a class="headerlink" href="#q-a" title="Permalink to this headline">#</a></h1>
<section id="gelu">
<h2>GELU를 사용한 이유<a class="headerlink" href="#gelu" title="Permalink to this headline">#</a></h2>
<p>안녕하세요, 논문읽으면서 궁금한 부분 공유드립니다.
논문 4페이지에서 ‘The MLP contains two layers with a GELU non-linearity.’ MLP 단에서 활성화 함수로 GELU를 사용했다고 하는데, 여러 활성화 함수 중에서 왜 선택하였는지는 언급이 없는 것 같네요. 제 생각은 아래 2개 정도로 생각이 드는데, 아마 다른 이유도 있겠죠?</p>
<ol class="simple">
<li><p>ReLU, ELU 등 다른 활성화 함수와 비교해서 성능을 확인해보았을 때 GELU의 성능이 더 좋았다.</p></li>
<li><p>아니면 BERT, GPT등에서 사용되는 함수여서 동일하게 적용해보았다.</p></li>
</ol>
<p>참고자료(Activation functions 에 대한 소개 글) :</p>
<p><a class="reference external" href="https://mlfromscratch.com/activation-functions-explained/#/">https://mlfromscratch.com/activation-functions-explained/#/</a></p>
<blockquote>
<div><p>저도 1, 2 같은 생각입니다. ViT가 나타나기 이전에 BERT, GPT 등 여러 Transformer 네트워크에서 다른 actiation function보다 좋은 성능을 보여주었기 때문에 당연하게(?) 사용되지 않았을까라는 생각이드네요.</p>
</div></blockquote>
<blockquote>
<div><p>당시에 SOTA 모델들이 GELU를 많이 사용했다라고 읽었던거 같아요 그래서 보증된 효과라 생각해서 쓴거라 생각합니다</p>
</div></blockquote>
</section>
<section id="cls-token-pe">
<h2>cls_token와 pe를 추가하는 순서<a class="headerlink" href="#cls-token-pe" title="Permalink to this headline">#</a></h2>
<p>다음 코드에서 함수 _pos_embed는 pe를 더하고 cls_token을 추가하는 것과 cls_token을 추가하고 pe를 더하는 것, 두 가지를 구현해놓았습니다.
두 가지의 차이가 뭘까요?</p>
<p><a class="reference external" href="https://github.com/rwightman/pytorch-image-models/blob/a520da9b495422bc773fb5dfe10819acb8bd7c5c/timm/models/vision_transformer.py#L430-L443">https://github.com/rwightman/pytorch-image-models/blob/a520da9b495422bc773fb5dfe10819acb8bd7c5c/timm/models/vision_transformer.py#L430-L443</a></p>
<blockquote>
<div><p>DeiT-3를 사용할때 PE를 더하고 cls_token을 추가해야하는 것 같습니다… 구체적인 내용은 모르겠네요;;<br />
<a class="reference external" href="https://github.com/rwightman/pytorch-image-models/commit/7d4b3807d5c40b0f8d7e66d27a7672684e482996">https://github.com/rwightman/pytorch-image-models/commit/7d4b3807d5c40b0f8d7e66d27a7672684e482996</a></p>
</div></blockquote>
</section>
<section id="fine-tuning-and-higher-resolution">
<h2>논문 3.2절 FINE-TUNING AND HIGHER RESOLUTION<a class="headerlink" href="#fine-tuning-and-higher-resolution" title="Permalink to this headline">#</a></h2>
<p>3.2 절의 FINE-TUNING AND HIGHER RESOLUTION를 다시 읽다가</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">We</span> <span class="n">therefore</span> <span class="n">perform</span> <span class="mi">2</span><span class="n">D</span> <span class="n">interpolation</span> <span class="n">of</span> <span class="n">the</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">position</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">according</span> <span class="n">to</span> <span class="n">their</span> <span class="n">location</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">original</span> <span class="n">image</span><span class="o">.</span>
</pre></div>
</div>
<p>라는 말이 있는데</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2</span><span class="n">D</span> <span class="n">interpolation</span> <span class="n">of</span> <span class="n">the</span> <span class="n">pre</span><span class="o">-</span><span class="n">trained</span> <span class="n">position</span> <span class="n">embeddings</span>
</pre></div>
</div>
<p>이 무엇을 의미하는 걸까요?</p>
<blockquote>
<div><p>제가 이해하기로는, 이전 문장에 ViT가 “임의의 sequence length를 입력받아 처리할 수 있지만, 그러면 pre-train된 position embedding이 의미 없어질 수 있다”라고 되어있습니다. 따라서 임의의 sequence length에 대해서는 pre-train된 position embedding을 사용하기 위해 interpolation하여 쓴다고 설명하고 있는 것 같습니다!</p>
</div></blockquote>
<blockquote>
<div><p>3.2절에서 수행한것이 기존 pre train보다 고차원 이미지를 fine tuning을 한것인데요. patch 크기는 기존 pre train과 동일한 크기이기 때문에 고차원인 경우 sequence length가 더 길어집니다. 그러면 기존 pre trained에서의 PE가 의미가 없어져서 고차원에서의 PE는 원본 이미지의 PE에 대응되는 것을 2d interpolation을 사용해서 구했다고 설명하는것 같네요</p>
</div></blockquote>
</section>
<section id="pe-vit-learnable">
<h2>왜 PE가 ViT에서 learnable한 파라미터로 구성되어있는가<a class="headerlink" href="#pe-vit-learnable" title="Permalink to this headline">#</a></h2>
<p>안녕하세요. ViT에서 PE를 추가하는 이유가 궁금해서 질문합니다. NLP에서는 순서 정보가 고려되어야 한다는점에서 PE를 추가해주는거 같은데, vision에서는 패치(토큰)가 이미지인데 패치간의 순서 정보까지 고려해야할 필요가 있는건가요? 왜 PE가 ViT에서 learnable한 파라미터로 구성하는지가 궁금합니다.</p>
<blockquote>
<div><p>3.1절의 Vision Transformer에서 Inductive Bias에 대한 내용 부분에 다음과 같이 작성되어있습니다.<br />
1.다양한 resolution의 이미지를 적응(?)시키기위해<br />
2.패치들끼리의 spatial relation을 학습시키기위해<br />
순서 정보와 공간 상의 위치 정보를 모두 고려하기위한 pe라고 이해하고 있습니다.
<img alt="" src="../../_images/02_4.png" /></p>
</div></blockquote>
<blockquote>
<div><p>PE없이도 학습이 될 수는 있는데 없으면 자연어 처리의 bag of words 처럼 데이터(패치)들간의 순서/위치 정보가 없어지는 걸로 알고 있습니다.<br />
논문 17페이지의 Appendix D.4 와 Table8에 Positional embedding에 대한 실험결과가 있습니다.<br />
No PE, 1d PE, 2d PE, Relative PE 4가지에 대해 실험해보았는데 PE가 없을때 보다 있을때가 성능이 좋았고 PE가 있을때는 설정바꾸어서 실험했을때 성능 차이가 거의 없어서 1d PE로 연구를 진행했다고 하네요.</p>
</div></blockquote>
<blockquote>
<div><p>보통 computer vision에서 이미지는 인접한 pixel간의 dependency가 더 크고 멀리 떨어져 있는 pixel간에는 dependency가 더 작다고 가정하고 문제를 푸는 경우가 많습니다. 이와 비슷하게 인접한 patch들간에 dependency가 더 크고, 멀리 떨어져 있는 patch일수록 dependency가 더 작다고 볼 수 있습니다. 어떤 patch가 가까이 있고, 멀리 있는지 이해하려면 위치 정보가 필요한데 PE를 사용하면 이러한 정보를 활용할 수 있겠죠. 결과적으로 PE를 썼을 때 성능이 더 잘 나오는 이유도 이러한 측면에서 향상된 게 아닐까 생각됩니다.<br />
PE를 썼을 때 성능이 개선되는 이유에는 여러 가지가 있겠지만 제가 당장 생각나는 것은 이정도네요.</p>
</div></blockquote>
<blockquote>
<div><p>ViT에서 patch embedding을 하고 self-attention 연산이 일어나는데 self-attention에서는 각 patch(Q)와 다른 patch들(K)과의 attention을 학습되는 것으로 알고 있습니다. 그렇다면 각 patch들의 거리와는 상관없이 어떤 위치의 patch들을 더 attention해야할지를 학습할 수 있게하기위해서 PE를 추가하는 것 아닐까요?</p>
</div></blockquote>
<blockquote>
<div><p>위치정보가 필요한 이유에 대해서 예시로 설명해보려고 했던건데 제가 괜히 혼란을 드린 게 아닌가 싶네요.ㅎㅎ;
말씀하신대로 PE는 여러 개의 patch중에서 attention이 높은 patch가 sequence에서 어디에 위치하고 있는지 학습하기 위한 정보로 사용된다고 하는 게 더 적절한 설명 같습니다.</p>
</div></blockquote>
</section>
<section id="patch-size">
<h2>논문 3.2절 patch size를 증가/감소시키는 것에 따른 장단점<a class="headerlink" href="#patch-size" title="Permalink to this headline">#</a></h2>
<p>안녕하세요. Vision Transformer 논문에서 3.2절을 읽다가 궁금한 점이 있어서 같이 얘기 나눠보면 좋을 거 같아서 질문드립니다.</p>
<p>3.2 FINE-TUNING AND HIGHER RESOLUTION
It is often beneficial to fine-tune at higher resolution than pre-training (Touvron et al., 2019; Kolesnikov et al., 2020). When feeding images of higher resolution, we keep the patch size the same, which results in a larger effective sequence length.</p>
<p>논문에서는 fine-tuning시 high resolution을 사용하되 patch size는 동일하게 하여 좀 더 긴 sequence length를 사용합니다. sequence length를 늘리는 것이 어떤 장점이 있는지 궁금합니다. resolution을 키웠으니까 patch size 또한 그만큼 증가시켜 사용하는 것도 타당해보이는데요. patch size가 작아지면 patch 단위에서의 receptive field는 작아지겠지만 어차피 transformer는 self-attention을 통해 global context 정보를 보기 때문에 receptive field는 문제되지 않을 거 같다는 생각이 듭니다. 그렇다면 sequence length를 늘리는 것은 어떤 장점이 있을까요?</p>
<p>결국 Vision Transformer에서 patch size를 증가/감소시키는 것에 따라 어떤 장단점이 있는지 궁금합니다.</p>
<blockquote>
<div><p>저도 비슷하게 resolution에 따른 성능 차이에 대해 궁금해서 찾아본 적이 있었는데요. 한 가지 예시로 다음 논문에서는 다음과 같이 말합니다.
<a class="reference external" href="https://arxiv.org/pdf/2105.04515.pdf">https://arxiv.org/pdf/2105.04515.pdf</a>
<img alt="" src="../../_images/02_5.png" />
“resolution이 작을 경우 정보가 너무 적다”라는 말로 이해를 했습니다. 그래서 high resolution의 경우에는 더 많은 정보를 담고 있지만 low resolution에서는 너무 적은 정보를 가지고 있어서 학습하기 어렵습니다. 그리고 low resolution에서는 aumentation을 적용해서 이런 단점을 보완하고 high resolution에서는 오히려 augmentation을 적용했을때 성능이 낮게 나왔다는 실험 결과도 있었습니다. (태스크마다 차이는 있을 것 같습니다.)<br />
저는 질문주신 내용에서 긴 sequence length를 사용한다는 것은 더 많은 정보를 활용한다는 것으로 이해할 수 있을 것 같고 그에 맞게 patch size를 tuning하여 적절한 값을 찾아야 합니다. 결국 patch size는 hyperparameter라고 생각해야하지 않을까 생각합니다.</p>
</div></blockquote>
<blockquote>
<div><p>말씀해주신 부분을 참고해서 곰곰히 생각해보니 제가 놓치고 있던 부분이 하나 있었네요. transformer에서는 patch size와 무관하게 모든 patch들을 일정한 크기의 벡터로 embedding하는데 이때 patch의 개수가 많으면 많을수록 embedding 이후 self-attention에서 활용할 수 있는 정보가 많겠지만 반대로 patch size를 늘리게 되면 그만큼 patch 개수가 줄어들어 self-attention에서 활용할 수 있는 정보가 제한적이겠네요.
patch size를 줄여서 self-attention에서 사용할 수 있는 토큰 개수를 늘리는 방식을 취했을 때, patch size를 어디까지 줄일 수 있는지 궁금하네요ㅎㅎ. 저도 patch size는 하이퍼 파라미터라고 생각합니다. 그래서 patch size를 조절한다면, embedding 차원, self-attention의 head 개수도 같이 고려해서 튜닝하는 게 필요하다고 생각이 드네요. 첨부해주신 논문도 읽어보도록 하겠습니다. 감사합니다.</p>
</div></blockquote>
<blockquote>
<div><p>안녕하세요, 저는 resolution이 커져도 같은 patch size를 사용하는 이유를 transformer self attention의 long term dependency학습능력을 최대로 활용하기 위해서라고 이해했습니다. patch size가 resolution과 관계없이 일정하다면, 큰 해상도에서 상대적으로 멀리 떨어져 있는 patch들 사이의 관계까지 학습할 수 있을 것 같습니다. 단순히 제 생각이지만 추후에 관련자료를 찾게되면 공유하겠습니다.</p>
</div></blockquote>
</section>
<section id="attention-distance">
<h2>4.5절의 attention distance의 의미<a class="headerlink" href="#attention-distance" title="Permalink to this headline">#</a></h2>
<p>우선 figure 6에 있는 attention map를 보겠습니다.
<img alt="" src="../../_images/02_6.png" />
attention map을 뽑아서 시각화를 하기위해서는 다음과 같이 각 레이어의 attention matrix를 가져옵니다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span><span class="p">,</span> <span class="n">att_mat</span><span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>

<span class="n">att_mat</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">att_mat</span><span class="p">)</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Average the attention weights across all heads.</span>
<span class="n">att_mat</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">att_mat</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># To account for residual connections, we add an identity matrix to the</span>
<span class="c1"># attention matrix and re-normalize the weights.</span>
<span class="n">residual_att</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">att_mat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">aug_att_mat</span><span class="o">=</span> <span class="n">att_mat</span><span class="o">+</span> <span class="n">residual_att</span>
<span class="n">aug_att_mat</span><span class="o">=</span> <span class="n">aug_att_mat</span><span class="o">/</span> <span class="n">aug_att_mat</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Recursively multiply the weight matrices</span>
<span class="n">joint_attentions</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">aug_att_mat</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
<span class="n">joint_attentions</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span> <span class="n">aug_att_mat</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

<span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">aug_att_mat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
    <span class="n">joint_attentions</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">aug_att_mat</span><span class="p">[</span><span class="n">n</span><span class="p">],</span> <span class="n">joint_attentions</span><span class="p">[</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

<span class="c1"># Attention from the output token to the input space.</span>
<span class="n">v</span><span class="o">=</span> <span class="n">joint_attentions</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">grid_size</span><span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">aug_att_mat</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
<span class="n">mask</span><span class="o">=</span> <span class="n">v</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">grid_size</span><span class="p">,</span> <span class="n">grid_size</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">mask</span><span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">mask</span><span class="o">/</span> <span class="n">mask</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">im</span><span class="o">.</span><span class="n">size</span><span class="p">)[</span><span class="o">...</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
<span class="n">result</span><span class="o">=</span> <span class="p">(</span><span class="n">mask</span><span class="o">*</span> <span class="n">im</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>이때 attention matrix은 평균을 내서 attention weight를 얻습니다. 바로 이 attention weight가 4.5절에 나오는 attention weight를 말합니다.</p>
<p>이제 각 레이어마다 attention map을 뽑아볼때 다음과 같이 나온다고 합시다.</p>
<p><img alt="" src="../../_images/02_7.png" />
<img alt="" src="../../_images/02_8.png" />
<img alt="" src="../../_images/02_9.png" />
<img alt="" src="../../_images/02_10.png" />
<img alt="" src="../../_images/02_11.png" />
<img alt="" src="../../_images/02_12.png" />
<img alt="" src="../../_images/02_13.png" />
<img alt="" src="../../_images/02_14.png" />
<img alt="" src="../../_images/02_15.png" />
<img alt="" src="../../_images/02_17.png" />
<img alt="" src="../../_images/02_18.png" /></p>
<p>오른쪽 layer마다 attention map을 뽑은 것을 뽑았을때 첫 번째 레이어는 12번째 레이어보다 밝은 부분이 여기저기 퍼져있는 것을 확인할 수 있습니다. 이렇게 밝은 부분이 넓게 분포되어있는 것은 attention하는 부분의 거리가 길다고도 말할 수 있습니다. 그리고 12번째 레이어는 밝은 부분이 한 곳에 잘 분포되어있는데 이것은 attention하는 부분의 거리가 짧다고 말할 수 있습니다.</p>
<p>이렇게 attention하는 부분의 거리가 길다, 짧다라고 말하는 부분이 attention distance에 대한 내용이고 이 attention distance와 layer depth에 대한 그래프를 다음과 같이 나타낸 것 같습니다.</p>
<p><img alt="" src="../../_images/02_19.png" /><br />
attention map을 뽑는 자세한 코드는 다음 링크에서 확인 하실 수 있습니다.</p>
<p>[Reference]<br />
<a class="reference external" href="https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb">https://github.com/jeonsworld/ViT-pytorch/blob/main/visualize_attention_map.ipynb</a></p>
<hr class="docutils" />
<p>Edit by <code class="docutils literal notranslate"><span class="pre">김주영</span></code></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/ch2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="02_code.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Vision Transformer Code</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../ch3/03_List.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pyramid Vision Transformer</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By PseudoLab<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>