
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Vision Transformer &#8212; Vision Transformer의 모든 것</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="shortcut icon" href="../../_static/PseudoLab_logo.png"/>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Vision Transformer Code" href="02_code.html" />
    <link rel="prev" title="Vision Transformer" href="02_List.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/PseudoLab_logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Vision Transformer의 모든 것</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Vision Transformer의 모든 것
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Inductive Bias와 Self-Attention
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch1/01_List.html">
   Inductive Bias와 Self-Attention
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/Inductive_Bias.html">
     Inductive Bias
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/Self-Attention.html">
     Self-Attention
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/01_code.html">
     Self-Attention Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch1/01_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Vision Transformer
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="02_List.html">
   Vision Transformer
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Vision Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_code.html">
     Vision Transformer Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="02_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Pyramid Vision Transformer
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../ch3/03_List.html">
   Pyramid Vision Transformer
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/Pyramid_Vision_Transformer.html">
     Pyramid Vision Transformer
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_code.html">
     Pyramid Vision Transformer Code
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ch3/03_qa.html">
     Q&amp;A
    </a>
   </li>
  </ul>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/pseudo-lab/All-About-ViT"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/pseudo-lab/All-About-ViT/issues/new?title=Issue%20on%20page%20%2Fdocs/ch2/vit.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/docs/ch2/vit.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   1. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#related-work">
   2. Related Work
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#method">
   3. Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vision-transformer-vit">
     1) Vision Transformer(ViT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inductive-bias">
     Inductive bias(=유도 편향)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hybrid-architecture">
     Hybrid Architecture
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fine-tuning-and-higher-resolution">
     2) Fine-tuning and higher Resolution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiments">
   4. Experiments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   5. Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-variants">
   6. Model Variants
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparison-to-state-of-the-art">
   7. Comparison to State Of The Art
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pre-training-data-requirements">
   8. Pre-training Data Requirements
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scaling-study">
   9. Scaling Study
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inspecting-vision-transformer">
   10. Inspecting Vision Transformer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   11. Conclusion
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Vision Transformer</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#abstract">
   Abstract
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   1. Introduction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#related-work">
   2. Related Work
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#method">
   3. Method
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vision-transformer-vit">
     1) Vision Transformer(ViT)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inductive-bias">
     Inductive bias(=유도 편향)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hybrid-architecture">
     Hybrid Architecture
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#fine-tuning-and-higher-resolution">
     2) Fine-tuning and higher Resolution
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#experiments">
   4. Experiments
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   5. Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-variants">
   6. Model Variants
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#comparison-to-state-of-the-art">
   7. Comparison to State Of The Art
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pre-training-data-requirements">
   8. Pre-training Data Requirements
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#scaling-study">
   9. Scaling Study
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inspecting-vision-transformer">
   10. Inspecting Vision Transformer
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   11. Conclusion
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="vision-transformer">
<h1>Vision Transformer<a class="headerlink" href="#vision-transformer" title="Permalink to this headline">#</a></h1>
<section id="abstract">
<h2>Abstract<a class="headerlink" href="#abstract" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Transformer 구조는 NLP task에서 굉장히 많이 쓰이지만 Vision task에서는 많이 사용되지 않음.</p></li>
<li><p>Vision에서는 Attention을 ConvNet에 결합해서 사용하거나 ConvNet의 일부분을 교체하여 사용함.</p></li>
<li><p>이 논문은 Attention 기법이 더 이상 CNN에 의존 할 필요가 없다는 것을 보여줌.</p></li>
<li><p>순수 Transformer를 image patch에 적용했을 때 Image classification 분야에서 좋은 성능을 냄.</p></li>
<li><p>대량의 dataset으로 pretrain 하고 소량의 특정 dataset으로 학습시키면 SOTA CNN보다 더 좋은 결과를 얻을 수 있고, 학습에 필요한 연산 리소스도 더 적다.</p></li>
<li><p>학습 후 용량은 대략 300MB 정도 됨.</p></li>
</ul>
</section>
<section id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Transformer는 연산 효율과 확장성 때문에 모델과 데이터셋이 계속 커져도 saturation이 발생하지 않음.</p></li>
<li><p>Transformer가 CNN 전체를 대체하기도 했었는데, 이 방법은 이론적으로는 효율적이지만, 특별한 Attention 패턴을 사용하기 때문에 하드웨어 가속기에서 효율적으로 확장 되지 못함.</p></li>
<li><p>이 논문은 최소한의 수정을 거친 Transformer 구조를 이미지에 직접 적용하기 위해 이미지를 패치로 나누고, 이 패치들의 선형 임베딩 시쿼스를 Transformer에 입력으로 제공했음, 그리고 Supervised 방식으로 image classification을 학습함.</p></li>
<li><p>Transformer는 CNN이 가진 Traslation Equivariance(변환 등변성)과 Locality(지역성) 등의 Inductive Bias를 안가지고 있기 때문에 training dataset의 양이 적으면 모델이 일반화 되지 않음.</p></li>
<li><p>ImageNet과 같은 작은 데이터셋으로 학습하면 CNN 모델의 성능이 더 좋지만, Inductive Bias를 무시할 수 있을 정도의 JFT-300M과 같은 큰 데이터셋으로 pretraining 하고 해결하려는 과제의 데이터셋에 transfer 시켰을 때에는 ViT(Vision Transformer)가 SOTA CNN 모델보다 더 좋은 성능을 냄.</p></li>
</ul>
</section>
<section id="related-work">
<h2>2. Related Work<a class="headerlink" href="#related-work" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Transformer는 기계번역을 위해 제안되었고, NLP 과제에서 SOTA를 달성했음.</p></li>
<li><p>대형 Transformer 기반의 모델들은 대량의 데이터셋으로 pretraining 되고 다른 작업에 fine-tune 됨.</p></li>
<li><p>BERT의 경우 Transformer를 denoising(잡음제거) self-supervised pretrain task에 사용됨.</p></li>
<li><p>GPT 라인의 모델들은 Transformer를 언어 모델링에 사용함.</p></li>
<li><p>Self-Attention을 image에 적용하기 위해서는 각 픽셀이 다른 모든 픽셀에 attend(집중)해야함 그런데 이 방법은 픽셀 수에 대해 Quadratic한 비용(2차 함수적 비용, U자 곡선을 그리는 비용을 말하는 듯)을 가지기 때문에 현실적으로 다양한 입력 사이즈로 스케일 되지 못함.</p></li>
<li><p>Transformer의 입력을 여러 사이즈로 스케일링 하기 위해 여러가지의 근사 방법들이 사용됨.</p>
<ol class="simple">
<li><p>self-attention을 인접 지역에만 적용하는 Local multi-head dot-product sefl-attention 적용.
→ Convolution들을 완벽하게 대체할 수 있게 됨.</p></li>
<li><p>Spars Transformer를 이미지에 적용.</p></li>
<li><p>Attention을 다양한 사이즈의 블럭에 적용.</p></li>
</ol>
</li>
<li><p>위 방법들은 CV task에선 좋은 결과를 보여줬지만 GPU에서 효율적으로 구현되기 힘들었음.</p></li>
<li><p>이미지를 2x2 픽셀의 패치로 나누어 Full self-attention을 적용한 모델이 ViT와 가장 흡사 하지만 이 모델의 경우 낮은 해상도에서만 사용 가능함.</p></li>
<li><p>CNN과 self-attention을 결합하기 위해 여러가지 방법들이 시도되었음.</p>
<ol class="simple">
<li><p>Self-attention을 이용하여 Feature map을 augmentation함.</p></li>
<li><p>Self-attnetion을 이용하여 CNN의 output에 추가적인 처리를 함.</p></li>
</ol>
</li>
</ul>
</section>
<section id="method">
<h2>3. Method<a class="headerlink" href="#method" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>NLP Transformer가 굉장히 효율적으로 구성되어 있어서 Image에도 즉시 적용 가능 하기 때문에 original-Transformer에 최대한 가깝게 모델을 디자인함.</p></li>
</ul>
<section id="vision-transformer-vit">
<h3>1) Vision Transformer(ViT)<a class="headerlink" href="#vision-transformer-vit" title="Permalink to this headline">#</a></h3>
<p><img alt="" src="../../_images/vit_0.png" /></p>
<ul>
<li><p>Transformer는 1차원 임베딩 토큰 시퀀스를 입력으로 받음.</p></li>
<li><p>Image를 Transformer의 입력으로 주기 위해</p>
<ol class="simple">
<li><p>Image를 패치단위로 나눔.
<span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{H\times W\times C}\)</span> ⇒ <span class="math notranslate nohighlight">\(\mathbf{x} \in \mathbb{R}^{N\times(P^2\centerdot C)}\)</span> ,이때 <span class="math notranslate nohighlight">\(N = HW/P^2\)</span>
H=높이, W=길이, C=채널, P=패치 해상도, N=패치 개수(입력 시퀀스의 길이)</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PatchEmbed</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img_size</span><span class="p">,</span> <span class="n">patch_size</span><span class="p">,</span> <span class="n">in_chans</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="mi">768</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PatchEmbed</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">img_size</span> <span class="o">=</span> <span class="n">img_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patch_size</span> <span class="o">=</span> <span class="n">patch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_patches</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_size</span> <span class="o">//</span> <span class="n">patch_size</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="n">in_chans</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
            <span class="n">stride</span><span class="o">=</span><span class="n">patch_size</span><span class="p">,</span>
        <span class="p">)</span>  <span class="c1"># Embedding dim으로 변환하며 패치크기의 커널로 패치크기만큼 이동하여 이미지를 패치로 분할 할 수 있음.</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># (batch_size, embed_dim, n_patches ** 0.5, n_patches ** 0.5)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 세번째 차원부터 끝까지 flatten (batch_size, embed_dim, n_patches)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>  <span class="c1"># (batch_size, n_patches, embed_dim)</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="c1"># 출처 : https://visionhong.tistory.com/25</span>
</pre></div>
</div>
<ol class="simple">
<li><p>각 패치를 flatten(평면화) 시킴.</p></li>
<li><p>Transformer는 모든 layer에 크기가 D인 상수 latent vector를 사용하기 때문에 Linear Projection(선형투영)을 사용하여 D차원으로 매핑.</p></li>
<li><p>Linear Projection의 결과물을 패치 임베딩으로 사용.</p></li>
</ol>
</li>
<li><p>BERT의 class 토큰과 비슷하게 임베딩된 패치들 맨앞에 학습 가능한 임베딩을 붙임.</p></li>
<li><p>이 임배딩 벡터<span class="math notranslate nohighlight">\((\mathbf{z}^0_{0}=\mathbf{x}_{class})\)</span>는 Transformer의 여러 Encoder층을 거쳐 최종 output<span class="math notranslate nohighlight">\((\mathbf{z}^0_L)\)</span>으로 나올 때, 이미지에 대한 Representation Vector의 역할을 수행함.</p></li>
<li><p>Classification Head
→ Pretraining 중 일 때는 1개의 Hidden Layer를 가진 MLP로 구현됨.
→ Fine-tuning 중 일 때는 1개의 Linear Layer로 구현됨.</p></li>
<li><p>임베딩에 위치정보를 포함시키기 위해 학습 가능한 Position embedding을 패치 임베딩에 추가함.
→ 이 때, Position Embedding은 1D position embedding을 사용함, 2D는 성능 향상이 없었기 때문.</p></li>
<li><p>위의 방법들로 구성된 최종 임베딩 벡터 시퀀스는 Encoder 의 입력이 됨.</p>
<p><img alt="" src="../../_images/vit_1.png" /></p>
<p><img alt="" src="../../_images/vit_2.png" /></p>
<p><img alt="" src="../../_images/vit_3.png" /></p>
<ul class="simple">
<li><p>Transformer의 입력은 시퀀스이기 때문에 분홍 부분이 낱개로 표현되어 있어도 1개로 보면됨.</p></li>
<li><p>각 각의 분홍색 앞에 붙은 숫자가 적힘 보라색이 Position Embedding.</p></li>
<li><p>*표시가 되어있는 분혹색이 Learnable Embedding이 되고 여기에도 Position Embedding이 붙음.</p></li>
</ul>
</li>
<li><p>그림에서도 볼 수 있듯이 Transformer Encoder는 Multi-head self-attention블록과 MLP 블록의 교차 레이어로 구성됨</p></li>
<li><p>Layer Norm은 모든 블록 이전에 적용되고, Residual connection은 모든 블록 이후에 적용됨.</p></li>
<li><p>MLP는 비선형 GELU 가 있는 2개의 Layer로 구성됨.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MLP</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MLP</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="p">,</span> <span class="n">hidden_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">act</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_features</span><span class="p">,</span> <span class="n">out_features</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
<span class="c1"># 출처 : https://visionhong.tistory.com/25</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="inductive-bias">
<h3>Inductive bias(=유도 편향)<a class="headerlink" href="#inductive-bias" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>ViT는 CNN보다 이미지별 Inductive bias(유도편향)이 적음.</p>
<ul>
<li><p>Why?</p>
<ul>
<li><p>이미지를 패치로 나누는 과정과 Fine-tuning 과정에서 position embedding을 조절 할 때만 사용되기 때문.(The two-dimensional neighborhood structure가 사용됨)</p></li>
<li><p>위의 두 과정을 제외하면 self-attention이 글로벌하게 사용되기 때문.</p></li>
</ul>
</li>
<li><p>CNN에서는 locality, two-dimensional neighborhood structure와 translation equivariance 등 지역적 관계성(local relation)을 기반으로하는 Inductive bias가 모델 전체에 걸쳐 사용됨.</p></li>
<li><p>Position Embedding 은 패치들의 2D position에 대한 정보를 제공하지 않음. 패치들 사이의 공간적 관계성은 처음부터 학습되어야 함.</p></li>
</ul>
</li>
</ul>
</section>
<section id="hybrid-architecture">
<h3>Hybrid Architecture<a class="headerlink" href="#hybrid-architecture" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>입력 시퀀스는 이미지를 CNN을 통과시켜 나온 feature map으로 만들어짐.</p></li>
<li><p>특별한 경우 패치의 공간 크기가 1x1이 될 수 있는데, 이는 입력 시퀀스가 feature map의 공간 차원을 평면화하고 Transformer 차워에 projection 함으로써 얻어진다는 것을 의미함.(?? 무슨말??)</p></li>
<li><p>Classification input embedding 과 position Embedding은 위에서 말한 것과 같음.
→ Encoder를 통과해서 나온 값에 Norm을 적용함.
→ 시퀀스 가장 앞에 붙였던 class token 만 따로 때서 classification head에 넣어줌. why? class token이 이미지 전체의 embedding을 표현하고 있음을 가정하기 때문.</p></li>
</ul>
</section>
<section id="fine-tuning-and-higher-resolution">
<h3>2) Fine-tuning and higher Resolution<a class="headerlink" href="#fine-tuning-and-higher-resolution" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>일반적으로 ViT는 큰 dataset에 pre-train시키고 후속 과제에 fine-tuning 시킴.</p></li>
<li><p>fine-tuning 과정에서 pretrain된 prediction head를 제거하고 0 initialized DxK feedforward layer를 부작함.(K는 fine-tuning 하는 dataset의 클래스 수)</p></li>
<li><p>pretrain 했을 때보다 높은 해상도의 dataset을 쓰면 더 좋기 때문에 이 논문에선 fine-tuning시 pretraining dataset 보다 큰 해상도의 dataset으로 진행함.</p></li>
<li><p>고해상도 이미지를 입력할 때 patch size는 똑같이 유지되므로 입력 시퀀스가 더 길어짐.</p></li>
<li><p>ViT는 어떠한 길이의 시퀀스도 처리할 수 있지만 pretrained position embedding의 기능을 잃게 됨.<br />
→ 이를 극복하고자 원본 이미지의 위치를 기준으로 하는 2D interpolation이 pretrained position embedding에 적용 됨.</p></li>
<li><p>위와 같은 해상도 조정과 patch extraction이 이미지의 2차원 구조에 대한 inductive bias가 수동으로 ViT에 주입되는 유일한 지점임.</p></li>
</ul>
</section>
</section>
<section id="experiments">
<h2>4. Experiments<a class="headerlink" href="#experiments" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>ResNet, ViT 그리고 Hybrid의 성능을 검증하기 위해 다양한 사이즈의 데이터셋에 pretrain 하고 벤치마크 task를 통해 검증함.</p></li>
<li><p>ViT는 대부분은 Recognition 벤치마크에서 적은 pretrain 비용으로 SOTA를 달성함.</p></li>
<li><p>최근에는 Self-Supervision을 이용하여 실험을 했는데, 이 실험 결과는 Self-Supervised ViT가 미래에 대한 가능성을 가짐을 보여줌.</p></li>
</ul>
</section>
<section id="dataset">
<h2>5. Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">#</a></h2>
<ul>
<li><p>모델 확장성(Scalability)를 알아보기 위해 여러가지 데이터셋을 사용함.</p>
<ol class="simple">
<li><p>ILSVRC-2012 ImageNet dataset(이하 ImageNet)</p>
<ul class="simple">
<li><p>클래스 : 1K</p></li>
<li><p>이미지 : 1.3M</p></li>
</ul>
</li>
<li><p>ImageNet-21k</p>
<ul class="simple">
<li><p>클래스 : 21K</p></li>
<li><p>이미지 : 14M</p></li>
</ul>
</li>
<li><p>JFT</p>
<ul class="simple">
<li><p>클래스 : 19K</p></li>
<li><p>이미지(고해상도 이미지) : 303M</p></li>
</ul>
</li>
</ol>
<p>⇒ 위 데이터셋을 사용하여 후보 모델들을 pretrain하고 다양한 벤치마크 task에 transfer 하여 모델 성능을 측정함.</p>
</li>
<li><p>벤치마크 데이터 셋</p>
<ol class="simple">
<li><p>ImageNet → pretrain에서 사용한 데이터들은 제외한 남은 holdout set으로 평가함.</p></li>
<li><p>ImageNet ReaL</p></li>
<li><p>CIFAR-10/100</p></li>
<li><p>Oxford-IIIT Pets</p></li>
<li><p>Oxford Flowers-102</p></li>
</ol>
</li>
<li><p>19개의 VTAB 분류 과제를 사용하여 검증.</p>
<ul class="simple">
<li><p>VTAB은 task당 1000개의 훈련 데이터를 사용하여 작은 데이터로 transfer 성능을 검증함.</p></li>
<li><p>Task들은 3개의 그룹으로 나뉨</p>
<ol class="simple">
<li><p>Natural - 위의 Pet와 CIFAR와 같은 task</p></li>
<li><p>Specialized - 의료 및 위성 이미지</p></li>
<li><p>Structured - 지역성과 같은 지리적 이해를 요구하는 task</p></li>
</ol>
</li>
</ul>
</li>
</ul>
</section>
<section id="model-variants">
<h2>6. Model Variants<a class="headerlink" href="#model-variants" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../_images/vit_4.png" /></p>
<ul class="simple">
<li><p>BERT의 구성요소를 base로 하였고, ViT-Base와 ViT-Large는 BERT와 완전 동일함.</p></li>
<li><p>더 큰 규모의 모델인 ViT-Huge도 추가함.</p></li>
<li><p>네이밍 방법</p>
<ul>
<li><p>ViT-L/16 → Patch size 16 * 16을 사용하는 Large 모델</p></li>
</ul>
</li>
<li><p>Transformer의 시퀀스 길이는 Patch size 제곱에 반비례 함 그러므로 작은 patch size를 쓰는 모델은 연산 비용이 더 비쌈.</p></li>
<li><p>Baseline CNN으로 ResNet을 사용하는데 Batch Normalization을 Group Normalization으로 교체하고 표준 Convolution을 사용함.</p></li>
<li><p>위와 같은 작은 수정을 거침으로서 transfer 성능이 올라가고 이런 수정된 모델 (modified model)을 ResNet(BiT - BigTransfer)라고 함.</p>
<ul>
<li><p>BiT - 큰 데이터셋을 transfer learning을 위해 pretrain 시키는 것.</p></li>
</ul>
</li>
<li><p>Hybrid에서는 CNN의 중간 Feature map들을 1x1 patch size로 쪼개어 ViT에 넣어줌.</p></li>
</ul>
</section>
<section id="comparison-to-state-of-the-art">
<h2>7. Comparison to State Of The Art<a class="headerlink" href="#comparison-to-state-of-the-art" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../_images/vit_5.png" /></p>
<ul class="simple">
<li><p>ViT-H/14 와 ViT-L/16을 SOTA CNN 모델들과 비교해봄.</p>
<ol class="simple">
<li><p>Large ResNet</p>
<ul>
<li><p>Supervised Transfer Learning을 적용한 BiT 모델</p></li>
</ul>
</li>
<li><p>Large EfficientNet</p>
<ul>
<li><p>Label이 없는 ImageNet과 JFT300M 데이터셋을 Semi-Supervised Learning으로 학습한 모델</p></li>
</ul>
</li>
</ol>
</li>
<li><p>JFT300M으로 pretrain 된 ViT-L/16 모델은 모든 task에서 BiT-L 보다 좋고, 연산 비용도 훨씬 낮음.</p></li>
<li><p>ViT-H/14 는 더 까다로운 데이터셋인 ImageNet과 CIFAR-100 그리고 VTAB에서 성능이 더욱 향상됨.</p></li>
<li><p>ViT 중 가장 큰 Huge 모델도 SOTA와 비교했을 때 pretrain 비용이 더 낮음.</p></li>
</ul>
</section>
<section id="pre-training-data-requirements">
<h2>8. Pre-training Data Requirements<a class="headerlink" href="#pre-training-data-requirements" title="Permalink to this headline">#</a></h2>
<ul>
<li><p>Dataset Size가 얼마나 큰 영향을 미치는 지를 보기 위해 2개의 실험을 함.</p>
<ol>
<li><p>ViT 모델을 점점 사이즈가 증가하는 Dataset으로 학습 시킴.</p>
<p><img alt="" src="../../_images/vit_6.png" /></p>
<ul class="simple">
<li><p>ImageNet → ImageNet 21K → JFT300M</p></li>
<li><p>작은 데이터셋에서 성능을 끌어올리기 위해 Weight Decay, Dropout, label Smoothing(정규화)을 적용함.</p></li>
<li><p>ImageNet과 같은 작은 데이터셋에선 ViT-B가 ViT-L보다 좋은 성능을 보여줌.</p></li>
<li><p>ImageNet21K의 경우 ViT-L과 B 모델의 성능이 비슷함.</p></li>
<li><p>JFT로 학습해야 Large 모델의 이점을 볼 수 있음.</p></li>
<li><p>ImageNet에선 BiT가 ViT보다 성능이 좋지만, JFT에선 ViT가 앞섬.</p></li>
</ul>
</li>
<li><p>Full JFT300M 데이터셋과 그 하위 데이터셋인 9M, 30M, 90M으로 학습 시킴.</p>
<p><img alt="" src="../../_images/vit_7.png" /></p>
<ul class="simple">
<li><p>모든 setting에 대해서 같은 hyper-parameter를 사용하여 규제의 효과가 아니라 본질적인 모델의 성능 평가를 함.</p></li>
<li><p>작은 데이터셋에선 ViT가 비슷한 크기의 BiT 보다 더 과적합됨.</p></li>
<li><p>ViT-B/32는 ResNet50 보다</p>
<ul>
<li><p>조금 더 빠름</p></li>
<li><p>9M에서는 성능이 더 안 좋음</p></li>
<li><p>90M 보다 큰 데이터셋에선 성능이 더 좋음</p></li>
</ul>
</li>
<li><p>이 결과는 Convolution Inductive Bias가 작은 데이터셋에서는 유용하지만, 큰 데이터셋에선 데이터로부터 직접적으로 유의미한 패턴을 학습하는 것으로 충분했고 심지어 Inductive Bias보다 유용함.</p></li>
</ul>
</li>
</ol>
</li>
</ul>
</section>
<section id="scaling-study">
<h2>9. Scaling Study<a class="headerlink" href="#scaling-study" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../_images/vit_8.png" /></p>
<ul class="simple">
<li><p>JFT300M에서 Transfer 성능을 평가하고 model performance VS Pretrain cost 를 비교 평가함.</p></li>
<li><p>BiT 모델 7개, Hybrid 모델 5개, ViT 모델 6개를 사용함.</p></li>
<li><p>위 그림을 보면 몇가지 특징을 알 수 있음.</p>
<ol class="simple">
<li><p>ViT는 성능/연산 Trade-Off에서 ResNet을 압도함.<br />
→ ViT는 ResNet보다 2 ~ 4배 낮은 연산을 하면서 같은 성능을 냄.</p></li>
<li><p>Hybrid가 작은 연산 비용에선 ViT를 앞서지만 큰 연산 비용에선 차이가 없어짐.<br />
→ 이 사실은 Convolution local feature processing이 ViT model size에 상관없이 도움을 준다는 것을 알 수 있음.</p></li>
<li><p>ViT는 이 논문을 쓴 연구원들이 시도했던 범위 내에선 Saturation이 나타나지 않았음.<br />
→ 모델 사이즈 키움으로써 더 성능을 높힐 수 있음</p></li>
</ol>
</li>
</ul>
</section>
<section id="inspecting-vision-transformer">
<h2>10. Inspecting Vision Transformer<a class="headerlink" href="#inspecting-vision-transformer" title="Permalink to this headline">#</a></h2>
<p><img alt="" src="../../_images/vit_9.png" /></p>
<p><img alt="" src="../../_images/vit_10.png" /></p>
<ul class="simple">
<li><p>ViT가 이미지를 어떻게 처리하는지 알기 위해 내부 representation들을 분석함.</p></li>
<li><p>ViT의 첫 번째 Layer는 Flatten된 Patch들을 저차원 공간에 Linearly-project(선형투영)함.</p></li>
<li><p>Projection 후에 학습된 Position Embedding이 patch representation에 추가됨.</p></li>
<li><p>위 그림(중앙)은 모델이 postion Embedding의 유사성으로 이미지 내에서 거리를 인코딩하는 방법을 학습하는 것을 보여 줌. 즉, 가까이 있는 patch 일수록 position embedding이 더 유사한 경향이 있음.</p></li>
<li><p>위 그림(중앙)을 보면 같은 row나 column에 있으면 비슷한 Embedding을 가지는 것을 알 수 있음.</p></li>
<li><p>ViT는 Self-Attention을 이용해서 최하층 layer(Input layer 다음 층)에서도 전체 이미지 정보를 통합 할 수 있음. 그래서 이 논문에선 네트워크가 Self-Attention을 어느정도 사용하는지 알아봄.</p></li>
<li><p>Attention weight를 기반으로 정보가 통합되는 이미지 공간의 평균 거리를 계산 함(그림7 오른쪽). 이 때 이 “Attention Distance”는 CNN의 수용 필드 크기 개념과 비슷함.</p></li>
<li><p>그림7의 오른쪽을 보면 몇몇 Head들은 이미 가장 낮은 layer에서 대부분의 이미지에 attend(집중)한다는 것을 발견 했는데 이것은 전체적인 정보 통합 기능이 모델에서 실제로 사용되고 있음을 보여줌.</p></li>
<li><p>하위층에서 몇 몇 Head들은 작은 거리를 갖는데, 이런 고도의 Localized Attention 은 Hybrid 모델에서 덜 나타남.</p></li>
<li><p>Localized Attention 은 CNN의 초기 Conv layer와 비슷한 기능을 하는 것을 알 수 있음.</p></li>
<li><p>실제로 모델이 분류에 의미적으로 관련 있는 영역에 Attend 한다는 사실을 발견함.</p></li>
</ul>
</section>
<section id="conclusion">
<h2>11. Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>ViT는 이전 연구들과는 다르게 초기 patch 추출 단계를 제외하고는 모델 구조에 Image Inductive Bias가 적용되지 않음.</p></li>
<li><p>Image Inductive Bias를 적용하지 않는 대신에 이미지를 Patch 시퀀스로 만들고 표준 Transformer Encoder에 넣어 처리함.</p></li>
<li><p>ViT는 SOTA 와 성능이 비슷하거나 더 좋지만, pretrain 비용은 상대적으로 낮음.</p></li>
<li><p>ViT의 연구 성과와 Carion의 연구(<a class="reference external" href="https://arxiv.org/pdf/2005.12872.pdf">논문 링크</a>)가 결합되면 Detection과 Segmentation에도 적용할 수 있을 것 같다고 함.</p></li>
</ul>
<hr class="docutils" />
<p>[Reference]<br />
<a class="reference external" href="https://kmhana.tistory.com/27">https://kmhana.tistory.com/27</a><br />
<a class="reference external" href="https://visionhong.tistory.com/25">https://visionhong.tistory.com/25</a><br />
<a class="reference external" href="https://velog.io/&#64;changdaeoh/Vision-Transformer-Review">https://velog.io/&#64;changdaeoh/Vision-Transformer-Review</a></p>
<hr class="docutils" />
<p>Author by <code class="docutils literal notranslate"><span class="pre">임중섭</span></code><br />
Edit by <code class="docutils literal notranslate"><span class="pre">김주영</span></code></p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/ch2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="02_List.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Vision Transformer</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="02_code.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Vision Transformer Code</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By PseudoLab<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>