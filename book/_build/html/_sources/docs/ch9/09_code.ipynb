{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compact Vision Transformers Code"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/SHI-Labs/Compact-Transformers/blob/main/src/cvt.py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CVT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 img_size=224,\n",
    "                 embedding_dim=768,\n",
    "                 n_input_channels=3,\n",
    "                 kernel_size=16,\n",
    "                 dropout=0.,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 num_layers=14,\n",
    "                 num_heads=6,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 positional_embedding='learnable',\n",
    "                 *args, **kwargs):\n",
    "        super(CVT, self).__init__()\n",
    "        assert img_size % kernel_size == 0, f\"Image size ({img_size}) has to be\" \\\n",
    "                                            f\"divisible by patch size ({kernel_size})\"\n",
    "        self.tokenizer = Tokenizer(n_input_channels=n_input_channels,\n",
    "                                   n_output_channels=embedding_dim,\n",
    "                                   kernel_size=kernel_size,\n",
    "                                   stride=kernel_size,\n",
    "                                   padding=0,\n",
    "                                   max_pool=False,\n",
    "                                   activation=None,\n",
    "                                   n_conv_layers=1,\n",
    "                                   conv_bias=True)\n",
    "\n",
    "        self.classifier = TransformerClassifier(\n",
    "            sequence_length=self.tokenizer.sequence_length(n_channels=n_input_channels,\n",
    "                                                           height=img_size,\n",
    "                                                           width=img_size),\n",
    "            embedding_dim=embedding_dim,\n",
    "            seq_pool=True,\n",
    "            dropout=dropout,\n",
    "            attention_dropout=attention_dropout,\n",
    "            stochastic_depth=stochastic_depth,\n",
    "            num_layers=num_layers,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            num_classes=num_classes,\n",
    "            positional_embedding=positional_embedding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenizer(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size, stride, padding,\n",
    "                 pooling_kernel_size=3, pooling_stride=2, pooling_padding=1,\n",
    "                 n_conv_layers=1,\n",
    "                 n_input_channels=3,\n",
    "                 n_output_channels=64,\n",
    "                 in_planes=64,\n",
    "                 activation=None,\n",
    "                 max_pool=True,\n",
    "                 conv_bias=False):\n",
    "        super(Tokenizer, self).__init__()\n",
    "\n",
    "        n_filter_list = [n_input_channels] + \\\n",
    "                        [in_planes for _ in range(n_conv_layers - 1)] + \\\n",
    "                        [n_output_channels]\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            *[nn.Sequential(\n",
    "                nn.Conv2d(n_filter_list[i], n_filter_list[i + 1],\n",
    "                          kernel_size=(kernel_size, kernel_size),\n",
    "                          stride=(stride, stride),\n",
    "                          padding=(padding, padding), bias=conv_bias),\n",
    "                nn.Identity() if activation is None else activation(),\n",
    "                nn.MaxPool2d(kernel_size=pooling_kernel_size,\n",
    "                             stride=pooling_stride,\n",
    "                             padding=pooling_padding) if max_pool else nn.Identity()\n",
    "            )\n",
    "                for i in range(n_conv_layers)\n",
    "            ])\n",
    "\n",
    "        self.flattener = nn.Flatten(2, 3)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def sequence_length(self, n_channels=3, height=224, width=224):\n",
    "        return self.forward(torch.zeros((1, n_channels, height, width))).shape[1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.flattener(self.conv_layers(x)).transpose(-2, -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(Module):\n",
    "    def __init__(self,\n",
    "                 seq_pool=True,\n",
    "                 embedding_dim=768,\n",
    "                 num_layers=12,\n",
    "                 num_heads=12,\n",
    "                 mlp_ratio=4.0,\n",
    "                 num_classes=1000,\n",
    "                 dropout=0.1,\n",
    "                 attention_dropout=0.1,\n",
    "                 stochastic_depth=0.1,\n",
    "                 positional_embedding='learnable',\n",
    "                 sequence_length=None):\n",
    "        super().__init__()\n",
    "        positional_embedding = positional_embedding if \\\n",
    "            positional_embedding in ['sine', 'learnable', 'none'] else 'sine'\n",
    "        dim_feedforward = int(embedding_dim * mlp_ratio)\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.seq_pool = seq_pool\n",
    "        self.num_tokens = 0\n",
    "\n",
    "        assert sequence_length is not None or positional_embedding == 'none', \\\n",
    "            f\"Positional embedding is set to {positional_embedding} and\" \\\n",
    "            f\" the sequence length was not specified.\"\n",
    "\n",
    "        if not seq_pool:\n",
    "            sequence_length += 1\n",
    "            self.class_emb = Parameter(torch.zeros(1, 1, self.embedding_dim),\n",
    "                                       requires_grad=True)\n",
    "            self.num_tokens = 1\n",
    "        else:\n",
    "            self.attention_pool = Linear(self.embedding_dim, 1)\n",
    "\n",
    "        if positional_embedding != 'none':\n",
    "            if positional_embedding == 'learnable':\n",
    "                self.positional_emb = Parameter(torch.zeros(1, sequence_length, embedding_dim),\n",
    "                                                requires_grad=True)\n",
    "                init.trunc_normal_(self.positional_emb, std=0.2)\n",
    "            else:\n",
    "                self.positional_emb = Parameter(self.sinusoidal_embedding(sequence_length, embedding_dim),\n",
    "                                                requires_grad=False)\n",
    "        else:\n",
    "            self.positional_emb = None\n",
    "\n",
    "        self.dropout = Dropout(p=dropout)\n",
    "        dpr = [x.item() for x in torch.linspace(0, stochastic_depth, num_layers)]\n",
    "        self.blocks = ModuleList([\n",
    "            TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads,\n",
    "                                    dim_feedforward=dim_feedforward, dropout=dropout,\n",
    "                                    attention_dropout=attention_dropout, drop_path_rate=dpr[i])\n",
    "            for i in range(num_layers)])\n",
    "        self.norm = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.fc = Linear(embedding_dim, num_classes)\n",
    "        self.apply(self.init_weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.positional_emb is None and x.size(1) < self.sequence_length:\n",
    "            x = F.pad(x, (0, 0, 0, self.n_channels - x.size(1)), mode='constant', value=0)\n",
    "\n",
    "        if not self.seq_pool:\n",
    "            cls_token = self.class_emb.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "\n",
    "        if self.positional_emb is not None:\n",
    "            x += self.positional_emb\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "\n",
    "        if self.seq_pool:\n",
    "            x = torch.matmul(F.softmax(self.attention_pool(x), dim=1).transpose(-1, -2), x).squeeze(-2)\n",
    "        else:\n",
    "            x = x[:, 0]\n",
    "\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TransformerEncoderLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(Module):\n",
    "    \"\"\"\n",
    "    Inspired by torch.nn.TransformerEncoderLayer and timm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1,\n",
    "                 attention_dropout=0.1, drop_path_rate=0.1):\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        self.pre_norm = LayerNorm(d_model)\n",
    "        self.self_attn = Attention(dim=d_model, num_heads=nhead,\n",
    "                                   attention_dropout=attention_dropout, projection_dropout=dropout)\n",
    "\n",
    "        self.linear1 = Linear(d_model, dim_feedforward)\n",
    "        self.dropout1 = Dropout(dropout)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.linear2 = Linear(dim_feedforward, d_model)\n",
    "        self.dropout2 = Dropout(dropout)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path_rate) if drop_path_rate > 0 else Identity()\n",
    "\n",
    "        self.activation = F.gelu\n",
    "\n",
    "    def forward(self, src: torch.Tensor, *args, **kwargs) -> torch.Tensor:\n",
    "        src = src + self.drop_path(self.self_attn(self.pre_norm(src)))\n",
    "        src = self.norm1(src)\n",
    "        src2 = self.linear2(self.dropout1(self.activation(self.linear1(src))))\n",
    "        src = src + self.drop_path(self.dropout2(src2))\n",
    "        return src"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Author by `김지훈`  \n",
    "Edit by `김주영`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8 (main, Oct 13 2022, 09:48:40) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
