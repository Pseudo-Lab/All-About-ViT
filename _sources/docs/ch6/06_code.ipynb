{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEiT: BERT Pre-Training of Image Transformers Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.modeling_discrete_vae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/unilm/blob/master/beit/modeling_discrete_vae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DallE discre VAE\n",
    "from dall_e import load_model\n",
    "\n",
    "\n",
    "class Dalle_VAE(BasicVAE):\n",
    "    def __init__(self, image_size):\n",
    "        super().__init__()\n",
    "        self.encoder = None\n",
    "        self.decoder = None\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def load_model(self, model_dir, device):\n",
    "        self.encoder = load_model(os.path.join(model_dir, \"encoder.pkl\"), device)\n",
    "        self.decoder = load_model(os.path.join(model_dir, \"decoder.pkl\"), device)\n",
    "\n",
    "    def decode(self, img_seq):\n",
    "        bsz = img_seq.size()[0]\n",
    "        img_seq = img_seq.view(bsz, self.image_size // 8, self.image_size // 8)\n",
    "        z = F.one_hot(img_seq, num_classes=self.encoder.vocab_size).permute(0, 3, 1, 2).float()\n",
    "        return self.decoder(z).float()\n",
    "\n",
    "    def get_codebook_indices(self, images):\n",
    "        z_logits = self.encoder(images)\n",
    "        return torch.argmax(z_logits, axis=1)\n",
    "\n",
    "    def get_codebook_probs(self, images):\n",
    "        z_logits = self.encoder(images)\n",
    "        return nn.Softmax(dim=1)(z_logits)\n",
    "\n",
    "    def forward(self, img_seq_prob, no_process=False):\n",
    "        if no_process:\n",
    "            return self.decoder(img_seq_prob.float()).float()\n",
    "        else:\n",
    "            bsz, seq_len, num_class = img_seq_prob.size()\n",
    "            z = img_seq_prob.view(bsz, self.image_size // 8, self.image_size // 8, self.encoder.vocab_size)\n",
    "            return self.decoder(z.permute(0, 3, 1, 2).float()).float()\n",
    "\n",
    "# Custom DiscreteVAE\n",
    "\n",
    "class DiscreteVAE(BasicVAE):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size = 256,\n",
    "        num_tokens = 512,\n",
    "        codebook_dim = 512,\n",
    "        num_layers = 3,\n",
    "        hidden_dim = 64,\n",
    "        channels = 3,\n",
    "        smooth_l1_loss = False,\n",
    "        temperature = 0.9,\n",
    "        straight_through = False,\n",
    "        kl_div_loss_weight = 0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # assert log2(image_size).is_integer(), 'image size must be a power of 2'\n",
    "        assert num_layers >= 1, 'number of layers must be greater than or equal to 1'\n",
    "\n",
    "        self.image_size = image_size\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_layers = num_layers\n",
    "        self.temperature = temperature\n",
    "        self.straight_through = straight_through\n",
    "        self.codebook = nn.Embedding(num_tokens, codebook_dim)\n",
    "\n",
    "        enc_layers = []\n",
    "        dec_layers = []\n",
    "\n",
    "        enc_in = channels\n",
    "        dec_in = codebook_dim\n",
    "\n",
    "        for layer_id in range(num_layers):\n",
    "            enc_layers.append(nn.Sequential(nn.Conv2d(enc_in, hidden_dim, 4, stride=2, padding=1), nn.ReLU()))\n",
    "            enc_layers.append(ResBlock(chan_in=hidden_dim, hidden_size=hidden_dim, chan_out=hidden_dim))\n",
    "            enc_in = hidden_dim\n",
    "            dec_layers.append(nn.Sequential(nn.ConvTranspose2d(dec_in, hidden_dim, 4, stride=2, padding=1), nn.ReLU()))\n",
    "            dec_layers.append(ResBlock(chan_in=hidden_dim, hidden_size=hidden_dim, chan_out=hidden_dim))\n",
    "            dec_in = hidden_dim\n",
    "\n",
    "        enc_layers.append(nn.Conv2d(hidden_dim, num_tokens, 1))\n",
    "        dec_layers.append(nn.Conv2d(hidden_dim, channels, 1))\n",
    "\n",
    "        self.encoder = nn.Sequential(*enc_layers)\n",
    "        self.decoder = nn.Sequential(*dec_layers)\n",
    "\n",
    "        self.loss_fn = F.smooth_l1_loss if smooth_l1_loss else F.mse_loss\n",
    "        self.kl_div_loss_weight = kl_div_loss_weight\n",
    "\n",
    "    def get_image_size(self):\n",
    "        return self.image_size\n",
    "\n",
    "    def get_image_tokens_size(self):\n",
    "        return self.image_size // 8\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def get_codebook_indices(self, images):\n",
    "        logits = self.forward(images, return_logits = True)\n",
    "        codebook_indices = logits.argmax(dim = 1)\n",
    "        return codebook_indices\n",
    "\n",
    "    @torch.no_grad()\n",
    "    @eval_decorator\n",
    "    def get_codebook_probs(self, images):\n",
    "        logits = self.forward(images, return_logits = True)\n",
    "        return nn.Softmax(dim=1)(logits)\n",
    "\n",
    "    def decode(\n",
    "        self,\n",
    "        img_seq\n",
    "    ):\n",
    "        image_embeds = self.codebook(img_seq)\n",
    "        b, n, d = image_embeds.shape\n",
    "        h = w = int(sqrt(n))\n",
    "\n",
    "        image_embeds = rearrange(image_embeds, 'b (h w) d -> b d h w', h = h, w = w)\n",
    "        images = self.decoder(image_embeds)\n",
    "        return images\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        img,\n",
    "        return_loss = False,\n",
    "        return_recons = False,\n",
    "        return_logits = False,\n",
    "        temp = None\n",
    "    ):\n",
    "        device, num_tokens, image_size, kl_div_loss_weight = img.device, self.num_tokens, self.image_size, self.kl_div_loss_weight\n",
    "        assert img.shape[-1] == image_size and img.shape[-2] == image_size, f'input must have the correct image size {image_size}'\n",
    "\n",
    "        logits = self.encoder(img)\n",
    "\n",
    "        if return_logits:\n",
    "            return logits # return logits for getting hard image indices for DALL-E training\n",
    "\n",
    "        temp = default(temp, self.temperature)\n",
    "        soft_one_hot = F.gumbel_softmax(logits, tau = temp, dim = 1, hard = self.straight_through)\n",
    "        sampled = einsum('b n h w, n d -> b d h w', soft_one_hot, self.codebook.weight)\n",
    "        out = self.decoder(sampled)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "        if not return_loss:\n",
    "            return out\n",
    "\n",
    "        # reconstruction loss\n",
    "\n",
    "        recon_loss = self.loss_fn(img, out)\n",
    "\n",
    "        # kl divergence\n",
    "\n",
    "        logits = rearrange(logits, 'b n h w -> b (h w) n')\n",
    "        qy = F.softmax(logits, dim = -1)\n",
    "\n",
    "        log_qy = torch.log(qy + 1e-10)\n",
    "        log_uniform = torch.log(torch.tensor([1. / num_tokens], device = device))\n",
    "        kl_div = F.kl_div(log_uniform, log_qy, None, None, 'batchmean', log_target = True)\n",
    "\n",
    "        loss = recon_loss + (kl_div * kl_div_loss_weight)\n",
    "\n",
    "        if not return_recons:\n",
    "            return loss\n",
    "\n",
    "        return loss, out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.masking_generator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/unilm/blob/master/beit/masking_generator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskingGenerator:\n",
    "    def __init__(\n",
    "            self, input_size, num_masking_patches, min_num_patches=4, max_num_patches=None,\n",
    "            min_aspect=0.3, max_aspect=None):\n",
    "        if not isinstance(input_size, tuple):\n",
    "            input_size = (input_size, ) * 2\n",
    "        self.height, self.width = input_size\n",
    "\n",
    "        self.num_patches = self.height * self.width\n",
    "        self.num_masking_patches = num_masking_patches\n",
    "\n",
    "        self.min_num_patches = min_num_patches\n",
    "        self.max_num_patches = num_masking_patches if max_num_patches is None else max_num_patches\n",
    "\n",
    "        max_aspect = max_aspect or 1 / min_aspect\n",
    "        self.log_aspect_ratio = (math.log(min_aspect), math.log(max_aspect))\n",
    "\n",
    "    def __repr__(self):\n",
    "        repr_str = \"Generator(%d, %d -> [%d ~ %d], max = %d, %.3f ~ %.3f)\" % (\n",
    "            self.height, self.width, self.min_num_patches, self.max_num_patches,\n",
    "            self.num_masking_patches, self.log_aspect_ratio[0], self.log_aspect_ratio[1])\n",
    "        return repr_str\n",
    "\n",
    "    def get_shape(self):\n",
    "        return self.height, self.width\n",
    "\n",
    "    def _mask(self, mask, max_mask_patches):\n",
    "        delta = 0\n",
    "        for attempt in range(10):\n",
    "            target_area = random.uniform(self.min_num_patches, max_mask_patches)\n",
    "            aspect_ratio = math.exp(random.uniform(*self.log_aspect_ratio))\n",
    "            h = int(round(math.sqrt(target_area * aspect_ratio)))\n",
    "            w = int(round(math.sqrt(target_area / aspect_ratio)))\n",
    "            if w < self.width and h < self.height:\n",
    "                top = random.randint(0, self.height - h)\n",
    "                left = random.randint(0, self.width - w)\n",
    "\n",
    "                num_masked = mask[top: top + h, left: left + w].sum()\n",
    "                # Overlap\n",
    "                if 0 < h * w - num_masked <= max_mask_patches:\n",
    "                    for i in range(top, top + h):\n",
    "                        for j in range(left, left + w):\n",
    "                            if mask[i, j] == 0:\n",
    "                                mask[i, j] = 1\n",
    "                                delta += 1\n",
    "\n",
    "                if delta > 0:\n",
    "                    break\n",
    "        return delta\n",
    "\n",
    "    def __call__(self):\n",
    "        mask = np.zeros(shape=self.get_shape(), dtype=np.int)\n",
    "        mask_count = 0\n",
    "        while mask_count < self.num_masking_patches:\n",
    "            max_mask_patches = self.num_masking_patches - mask_count\n",
    "            max_mask_patches = min(max_mask_patches, self.max_num_patches)\n",
    "\n",
    "            delta = self._mask(mask, max_mask_patches)\n",
    "            if delta == 0:\n",
    "                break\n",
    "            else:\n",
    "                mask_count += delta\n",
    "\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.modeling_pretrain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/microsoft/unilm/blob/master/beit/modeling_pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformerForMaskedImageModeling(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, vocab_size=8192, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, drop_rate=0., attn_drop_rate=0.,\n",
    "                 drop_path_rate=0., norm_layer=None, init_values=None, attn_head_dim=None,\n",
    "                 use_abs_pos_emb=True, use_rel_pos_bias=False, use_shared_rel_pos_bias=False, init_std=0.02, **kwargs):\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        if use_abs_pos_emb:\n",
    "            self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        else:\n",
    "            self.pos_embed = None\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "\t\t\t\t# https://jeonsworld.github.io/NLP/rel_pe/\n",
    "        if use_shared_rel_pos_bias:\n",
    "            self.rel_pos_bias = RelativePositionBias(window_size=self.patch_embed.patch_shape, num_heads=num_heads)\n",
    "        else:\n",
    "            self.rel_pos_bias = None\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                init_values=init_values, window_size=self.patch_embed.patch_shape if use_rel_pos_bias else None,\n",
    "                attn_head_dim=attn_head_dim,\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        self.init_std = init_std\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        if self.pos_embed is not None:\n",
    "            trunc_normal_(self.pos_embed, std=self.init_std)\n",
    "        trunc_normal_(self.cls_token, std=self.init_std)\n",
    "        trunc_normal_(self.mask_token, std=self.init_std)\n",
    "        trunc_normal_(self.lm_head.weight, std=self.init_std)\n",
    "        self.apply(self._init_weights)\n",
    "        self.fix_init_weight()\n",
    "\n",
    "    def fix_init_weight(self):\n",
    "        def rescale(param, layer_id):\n",
    "            param.div_(math.sqrt(2.0 * layer_id))\n",
    "\n",
    "        for layer_id, layer in enumerate(self.blocks):\n",
    "            rescale(layer.attn.proj.weight.data, layer_id + 1)\n",
    "            rescale(layer.mlp.fc2.weight.data, layer_id + 1)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            trunc_normal_(m.weight, std=self.init_std)\n",
    "            if m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def get_num_layers(self):\n",
    "        return len(self.blocks)\n",
    "\n",
    "    def forward_features(self, x, bool_masked_pos):\n",
    "        x = self.patch_embed(x, bool_masked_pos=bool_masked_pos)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        mask_token = self.mask_token.expand(batch_size, seq_len, -1)\n",
    "\n",
    "        # replace the masked visual tokens by mask_token\n",
    "        w = bool_masked_pos.unsqueeze(-1).type_as(mask_token)\n",
    "        x = x * (1 - w) + mask_token * w\n",
    "\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        if self.pos_embed is not None:\n",
    "            x = x + self.pos_embed\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        rel_pos_bias = self.rel_pos_bias() if self.rel_pos_bias is not None else None\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, rel_pos_bias=rel_pos_bias)\n",
    "\n",
    "        return self.norm(x)\n",
    "\n",
    "    def forward(self, x, bool_masked_pos, return_all_tokens=False):\n",
    "        x = self.forward_features(x, bool_masked_pos=bool_masked_pos)\n",
    "        x = x[:, 1:]\n",
    "        if return_all_tokens:\n",
    "            return self.lm_head(x)\n",
    "        else:\n",
    "            # return the masked tokens\n",
    "            return self.lm_head(x[bool_masked_pos])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Author by `박민식`  \n",
    "Edit by `김주영`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
