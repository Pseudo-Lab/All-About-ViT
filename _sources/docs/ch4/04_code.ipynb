{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeiT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, in_channels=3, patch_size=16, embbeding_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size # 16x16\n",
    "        self.n_patches = (img_size // patch_size) **2 # number of patches in image\n",
    "\n",
    "        self.proj = nn.Conv2d(in_channels,\n",
    "                              embbeding_dim,\n",
    "                              kernel_size=self.patch_size,\n",
    "                              stride=self.patch_size)\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, embbeding_dim))\n",
    "        self.dist_token = nn.Parameter(torch.rand(1, 1, embbeding_dim))\n",
    "        self.position_embedding = nn.Parameter(torch.rand(1, 2 + self.n_patches, embbeding_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "\n",
    "        x = self.proj(x) # (batch, embedding_dim, 14, 14)                \n",
    "        x = x.flatten(2) # (batch, embedding_dim, n_patches)        \n",
    "        x = x.transpose(1, 2) # (batch, n_patches, embedding_dim) \n",
    "\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        dist_token = self.dist_token.expand(x.shape[0], -1, -1)\n",
    "\n",
    "\n",
    "        # add class token, dist token        \n",
    "        x = torch.cat([cls_token, dist_token, x], dim=1) # (batch, n_patches + 2, embedding_dim)  \n",
    "\n",
    "        \n",
    "        # add position embedding        \n",
    "        position_embedding = self.position_embedding.expand(x.shape[0], -1, -1)\n",
    "        x = x + position_embedding\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads=12, qkv_bias=True, attn_p=0., proj_p=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = self.dim // n_heads\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_p)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_p)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape # (b, 198, 768)\n",
    "\n",
    "        qkv = self.qkv(x) # (b, 198, 768*3)\n",
    "        qkv = qkv.reshape(B, N, 3, self.n_heads, C // self.n_heads) # (b, 198, 768*3) -> (b, 198, 3, 12, 64)\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # (b, 198, 3, 12, 64) -> (3, b, 12, 198, 96)\n",
    "\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # (b, 12, 198, 64)\n",
    "\n",
    "        # q * k\n",
    "        attention = (q @ k.transpose(-2, -1)) * self.scale # (8, 12, 198, 198) * scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        attention = self.attn_drop(attention)\n",
    "\n",
    "        # attention * v\n",
    "        attention = (attention @ v).transpose(1, 2).reshape(B, N, C) # (b, 198, 768)\n",
    "        attention = self.proj(attention)\n",
    "        attention = self.proj_drop(attention)\n",
    "        \n",
    "        return attention\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, dim, expansion=4, p=0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(dim, dim*expansion)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc2 = nn.Linear(dim*expansion, dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)        \n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, expansion=4, qkv_bias=True, p=0., attn_p=0.):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attention = MultiHeadAttention(dim, \n",
    "                                            n_heads, \n",
    "                                            qkv_bias,\n",
    "                                            attn_p=attn_p,\n",
    "                                            proj_p=p)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, expansion=expansion, p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MLPHead(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.fc1 = nn.Linear(embedding_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.norm1(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DeiT(nn.Module):\n",
    "    def __init__(self, \n",
    "                img_size=224,\n",
    "                patch_size=16, \n",
    "                in_channels=3, \n",
    "                num_classes=1000, \n",
    "                embbeding_dim=768, \n",
    "                depth=12, \n",
    "                n_heads=8, \n",
    "                expansion=4, \n",
    "                qkv_bias=True,\n",
    "                p=0.,\n",
    "                attn_p=0.,\n",
    "                is_training=True):\n",
    "        super().__init__()\n",
    "        \n",
    "\n",
    "        self.is_training = is_training\n",
    "\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(img_size=img_size,\n",
    "                                              in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embbeding_dim=embbeding_dim)\n",
    "\n",
    "\n",
    "        self.enc_blocks = nn.ModuleList([EncoderBlock(dim=embbeding_dim,\n",
    "                                                    n_heads=n_heads,\n",
    "                                                    expansion=expansion,\n",
    "                                                    qkv_bias=qkv_bias,\n",
    "                                                    p=p,\n",
    "                                                    attn_p=attn_p)\n",
    "                                                    for _ in range(depth)])\n",
    "        \n",
    "        \n",
    "        self.mlp_cls = MLPHead(embedding_dim=embbeding_dim,\n",
    "                                num_classes=num_classes)\n",
    "        \n",
    "\n",
    "        self.mlp_dist = MLPHead(embedding_dim=embbeding_dim,\n",
    "                                num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        for encoder in self.enc_blocks:\n",
    "            x = encoder(x)\n",
    "            \n",
    "        cls_token_final = x[:, 0]\n",
    "        dist_token_final = x[:, 1]\n",
    "\n",
    "        x_cls = self.mlp_cls(cls_token_final)\n",
    "        x_dist = self.mlp_dist(dist_token_final)\n",
    "\n",
    "        if self.is_training:\n",
    "            return x_cls, x_dist\n",
    "        else: \n",
    "            # inference\n",
    "            return (x_cls + x_dist) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard distillation global loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Hard_Disitillation_Global_Loss(nn.Module):\n",
    "    def __init__(self, teacher, alpha, tau):\n",
    "        super(Hard_Disitillation_Global_Loss, self).__init__()\n",
    "\n",
    "        self.teacher = teacher\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "    \n",
    "    def forward(self, inputs, outputs_student, labels):        \n",
    "        cls_token, dist_token = outputs_student\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs_teacher = self.teacher(inputs)\n",
    "        \n",
    "        loss = ((1-self.alpha)* F.CrossEntropyLoss(cls_token, labels)) + (self.alpha * F.CrossEntropyLoss(dist_token, outputs_teacher.argmax(dim=1)))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.DeiT import DeiT\n",
    "from util.loss import *\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "    print(device)\n",
    "\n",
    "student = DeiT(img_size=224, \n",
    "            patch_size=16, \n",
    "            in_channels=3, \n",
    "            num_classes=1000, \n",
    "            embbeding_dim=768, \n",
    "            depth=12, \n",
    "            n_heads=8, \n",
    "            expansion=4, \n",
    "            qkv_bias=True, \n",
    "            p=0., \n",
    "            attn_p=0.,\n",
    "            training=True)    \n",
    "student.to(device)\n",
    "\n",
    "teacher = models.resnet50()\n",
    "teacher.to(device)\n",
    "\n",
    "# teacher weight freeze\n",
    "for params in teacher.parameters():    \n",
    "    params.requires_grad = False\n",
    "\n",
    "criterion = Hard_Disitillation_Global_Loss(teacher=teacher, alpha=0.5, tau=1).to(device)\n",
    "\n",
    "student.train()\n",
    "teacher.eval()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)    \n",
    "    for inputs, labels in loop:            \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs_student = student(inputs)\n",
    "        loss = criterion(inputs, outputs_student, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Author by `이명오`  \n",
    "Edit by `김주영`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
