## GELU를 사용한 이유

안녕하세요, 논문읽으면서 궁금한 부분 공유드립니다.
논문 4페이지에서 'The MLP contains two layers with a GELU non-linearity.' MLP 단에서 활성화 함수로 GELU를 사용했다고 하는데, 여러 활성화 함수 중에서 왜 선택하였는지는 언급이 없는 것 같네요. 제 생각은 아래 2개 정도로 생각이 드는데, 아마 다른 이유도 있겠죠?

1. ReLU, ELU 등 다른 활성화 함수와 비교해서 성능을 확인해보았을 때 GELU의 성능이 더 좋았다.
2. 아니면 BERT, GPT등에서 사용되는 함수여서 동일하게 적용해보았다.

참고자료(Activation functions 에 대한 소개 글) :

[https://mlfromscratch.com/activation-functions-explained/#/](https://mlfromscratch.com/activation-functions-explained/#/)

> 저도 1, 2 같은 생각입니다. ViT가 나타나기 이전에 BERT, GPT 등 여러 Transformer 네트워크에서 다른 actiation function보다 좋은 성능을 보여주었기 때문에 당연하게(?) 사용되지 않았을까라는 생각이드네요.
>

>당시에 SOTA 모델들이 GELU를 많이 사용했다라고 읽었던거 같아요 그래서 보증된 효과라 생각해서 쓴거라 생각합니다
>

## cls_token와 pe를 추가하는 순서

다음 코드에서 함수 _pos_embed는 pe를 더하고 cls_token을 추가하는 것과 cls_token을 추가하고 pe를 더하는 것, 두 가지를 구현해놓았습니다.
두 가지의 차이가 뭘까요?

[https://github.com/rwightman/pytorch-image-models/blob/a520da9b495422bc773fb5dfe10819acb8bd7c5c/timm/models/vision_transformer.py#L430-L443](https://github.com/rwightman/pytorch-image-models/blob/a520da9b495422bc773fb5dfe10819acb8bd7c5c/timm/models/vision_transformer.py#L430-L443)

> DeiT-3를 사용할때 PE를 더하고 cls_token을 추가해야하는 것 같습니다... 구체적인 내용은 모르겠네요;;  
https://github.com/rwightman/pytorch-image-models/commit/7d4b3807d5c40b0f8d7e66d27a7672684e482996
>

## 논문 3.2절 FINE-TUNING AND HIGHER RESOLUTION

3.2 절의 FINE-TUNING AND HIGHER RESOLUTION를 다시 읽다가

```
We therefore perform 2D interpolation of the pre-trained position embeddings, according to their location in the original image.
```

라는 말이 있는데

```
2D interpolation of the pre-trained position embeddings
```

이 무엇을 의미하는 걸까요?

>제가 이해하기로는, 이전 문장에 ViT가 "임의의 sequence length를 입력받아 처리할 수 있지만, 그러면 pre-train된 position embedding이 의미 없어질 수 있다"라고 되어있습니다. 따라서 임의의 sequence length에 대해서는 pre-train된 position embedding을 사용하기 위해 interpolation하여 쓴다고 설명하고 있는 것 같습니다!
>

>3.2절에서 수행한것이 기존 pre train보다 고차원 이미지를 fine tuning을 한것인데요. patch 크기는 기존 pre train과 동일한 크기이기 때문에 고차원인 경우 sequence length가 더 길어집니다. 그러면 기존 pre trained에서의 PE가 의미가 없어져서 고차원에서의 PE는 원본 이미지의 PE에 대응되는 것을 2d interpolation을 사용해서 구했다고 설명하는것 같네요
>

## 왜 PE가 ViT에서 learnable한 파라미터로 구성되어있는가

안녕하세요. ViT에서 PE를 추가하는 이유가 궁금해서 질문합니다. NLP에서는 순서 정보가 고려되어야 한다는점에서 PE를 추가해주는거 같은데, vision에서는 패치(토큰)가 이미지인데 패치간의 순서 정보까지 고려해야할 필요가 있는건가요? 왜 PE가 ViT에서 learnable한 파라미터로 구성하는지가 궁금합니다.

>3.1절의 Vision Transformer에서 Inductive Bias에 대한 내용 부분에 다음과 같이 작성되어있습니다.  
1.다양한 resolution의 이미지를 적응(?)시키기위해  
2.패치들끼리의 spatial relation을 학습시키기위해  
순서 정보와 공간 상의 위치 정보를 모두 고려하기위한 pe라고 이해하고 있습니다.
![](imgs/01_4.png)
>

>PE없이도 학습이 될 수는 있는데 없으면 자연어 처리의 bag of words 처럼 데이터(패치)들간의 순서/위치 정보가 없어지는 걸로 알고 있습니다.  
논문 17페이지의 Appendix D.4 와 Table8에 Positional embedding에 대한 실험결과가 있습니다.  
No PE, 1d PE, 2d PE, Relative PE 4가지에 대해 실험해보았는데 PE가 없을때 보다 있을때가 성능이 좋았고 PE가 있을때는 설정바꾸어서 실험했을때 성능 차이가 거의 없어서 1d PE로 연구를 진행했다고 하네요.
>

>보통 computer vision에서 이미지는 인접한 pixel간의 dependency가 더 크고 멀리 떨어져 있는 pixel간에는 dependency가 더 작다고 가정하고 문제를 푸는 경우가 많습니다. 이와 비슷하게 인접한 patch들간에 dependency가 더 크고, 멀리 떨어져 있는 patch일수록 dependency가 더 작다고 볼 수 있습니다. 어떤 patch가 가까이 있고, 멀리 있는지 이해하려면 위치 정보가 필요한데 PE를 사용하면 이러한 정보를 활용할 수 있겠죠. 결과적으로 PE를 썼을 때 성능이 더 잘 나오는 이유도 이러한 측면에서 향상된 게 아닐까 생각됩니다.  
PE를 썼을 때 성능이 개선되는 이유에는 여러 가지가 있겠지만 제가 당장 생각나는 것은 이정도네요.
>

>ViT에서 patch embedding을 하고 self-attention 연산이 일어나는데 self-attention에서는 각 patch(Q)와 다른 patch들(K)과의 attention을 학습되는 것으로 알고 있습니다. 그렇다면 각 patch들의 거리와는 상관없이 어떤 위치의 patch들을 더 attention해야할지를 학습할 수 있게하기위해서 PE를 추가하는 것 아닐까요?
>

>위치정보가 필요한 이유에 대해서 예시로 설명해보려고 했던건데 제가 괜히 혼란을 드린 게 아닌가 싶네요.ㅎㅎ;
말씀하신대로 PE는 여러 개의 patch중에서 attention이 높은 patch가 sequence에서 어디에 위치하고 있는지 학습하기 위한 정보로 사용된다고 하는 게 더 적절한 설명 같습니다.
>